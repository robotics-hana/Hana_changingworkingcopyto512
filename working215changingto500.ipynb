{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Experimentation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "FWumlmcL3r0a",
        "pqJrjauP846-",
        "DGvexk_lvq1b",
        "og7YgBCLct5T",
        "Wb_jgNjdurVl",
        "vDxQ-_Ao7NBG",
        "QJqa5zAobgr2",
        "IqNWFF1wyZtE",
        "EJbgqIFEvty2",
        "WrXjUZiI5oNH",
        "P5KsYCx34JdT",
        "PdPUGj0iCqX9",
        "hGZXjnnfiFU6",
        "HICnX_kEQDhR",
        "CKl_IU1q4yeE",
        "WDa_hakb42II",
        "ieuJPBse44cA"
      ],
      "gpuType": "L4"
    },
    "interpreter": {
      "hash": "26284b83044cf6d18497d2f2bcb8891a6475cbd87428bcdc4981d4fbeeb38628"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWumlmcL3r0a"
      },
      "source": [
        "# Load Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "yat"
      ],
      "metadata": {
        "id": "u4ppKRCyKTAT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xf8Iv5caGLEe"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install extra packages needed for this notebook\n",
        "!pip install -q pyunpack patool rarfile rioxarray\n",
        "\n",
        "# --- Imports ---\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import pathlib\n",
        "import requests\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "import numpy as np\n",
        "import rasterio\n",
        "from rasterio.warp import reproject, Resampling\n",
        "from rasterio.windows import Window\n",
        "import PIL\n",
        "from PIL import Image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import *\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import pyunpack\n",
        "from rarfile import RarFile\n",
        "import rioxarray as rxr\n",
        "\n",
        "from sklearn.metrics import *\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Conv2D, Conv2DTranspose, MaxPooling2D,\n",
        "    Dropout, BatchNormalization, Activation, UpSampling2D,\n",
        "    concatenate, add, multiply,\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g6QqYr9jKwcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjNYvVXlC6Ja"
      },
      "source": [
        "# --- 0. Setup: imports & paths ---\n",
        "\n",
        "# Folder where GEE exports were saved\n",
        "GEE_DIR = '/content/drive/MyDrive/GEE_Iraq'\n",
        "\n",
        "# Where to save tiles\n",
        "OUT_IMG_DIR = '/content/tiles/images'\n",
        "OUT_MSK_DIR = '/content/tiles/masks'\n",
        "\n",
        "os.makedirs(OUT_IMG_DIR, exist_ok=True)\n",
        "os.makedirs(OUT_MSK_DIR, exist_ok=True)\n",
        "\n",
        "TILE_SIZE = 512   # UNet input tile size\n",
        "MIN_WATER_PIXELS = 50  # skip almost-all-land tiles (adjust as you like)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def align_mask_to_image(img_path, mask_path):\n",
        "    \"\"\"\n",
        "    Returns (img_arr, mask_arr, profile) where:\n",
        "      img_arr:  (H, W, 4)\n",
        "      mask_arr: (H, W) with values 0/1\n",
        "    \"\"\"\n",
        "    with rasterio.open(img_path) as src_img:\n",
        "        img = src_img.read()  # (bands, H, W)\n",
        "        img_profile = src_img.profile\n",
        "\n",
        "    with rasterio.open(mask_path) as src_msk:\n",
        "        msk = src_msk.read(1)  # (H2, W2)\n",
        "\n",
        "        # If shapes + transforms match, no reprojection needed\n",
        "        if (src_msk.transform == img_profile['transform'] and\n",
        "            src_msk.width == img_profile['width'] and\n",
        "            src_msk.height == img_profile['height']):\n",
        "            mask_resampled = msk\n",
        "        else:\n",
        "            # Reproject to match image grid\n",
        "            mask_resampled = np.zeros((img_profile['height'], img_profile['width']), dtype=np.float32)\n",
        "\n",
        "            reproject(\n",
        "                source=msk,\n",
        "                destination=mask_resampled,\n",
        "                src_transform=src_msk.transform,\n",
        "                src_crs=src_msk.crs,\n",
        "                dst_transform=img_profile['transform'],\n",
        "                dst_crs=img_profile['crs'],\n",
        "                resampling=Resampling.nearest\n",
        "            )\n",
        "\n",
        "    # Move image channels last: (bands, H, W) -> (H, W, bands)\n",
        "    img_arr = np.transpose(img, (1, 2, 0)).astype(np.float32)\n",
        "\n",
        "    # Ensure mask is 0/1\n",
        "    mask_arr = (mask_resampled > 0.5).astype(np.uint8)\n",
        "\n",
        "    return img_arr, mask_arr, img_profile\n"
      ],
      "metadata": {
        "id": "x7ku80U8hlfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tile_image_and_mask(img_arr, mask_arr, base_name,\n",
        "                        tile_size=TILE_SIZE,\n",
        "                        min_water_pixels=MIN_WATER_PIXELS):\n",
        "    \"\"\"\n",
        "    Cuts image & mask into tiles, saves non-empty ones as .npy.\n",
        "    \"\"\"\n",
        "    H, W, C = img_arr.shape\n",
        "    tile_id = 0\n",
        "\n",
        "    for row in range(0, H, tile_size):\n",
        "        for col in range(0, W, tile_size):\n",
        "            if row + tile_size > H or col + tile_size > W:\n",
        "                continue  # skip incomplete edge tiles (optional)\n",
        "\n",
        "            img_tile = img_arr[row:row+tile_size, col:col+tile_size, :]\n",
        "            msk_tile = mask_arr[row:row+tile_size, col:col+tile_size]\n",
        "\n",
        "            # Skip tiles with almost no water\n",
        "            if msk_tile.sum() < min_water_pixels:\n",
        "                continue\n",
        "\n",
        "            img_out_path = os.path.join(\n",
        "                OUT_IMG_DIR, f\"{base_name}_tile_{tile_id:04d}.npy\"\n",
        "            )\n",
        "            msk_out_path = os.path.join(\n",
        "                OUT_MSK_DIR, f\"{base_name}_tile_{tile_id:04d}.npy\"\n",
        "            )\n",
        "\n",
        "            np.save(img_out_path, img_tile)\n",
        "            np.save(msk_out_path, msk_tile)\n",
        "\n",
        "            tile_id += 1\n",
        "\n",
        "    print(f\"{base_name}: saved {tile_id} tiles.\")\n"
      ],
      "metadata": {
        "id": "VM8RiaodhmbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find all S2 images\n",
        "s2_files = sorted(glob.glob(os.path.join(GEE_DIR, '*_s2_4band.tif')))\n",
        "\n",
        "for s2_path in s2_files:\n",
        "    base = os.path.basename(s2_path).replace('_s2_4band.tif', '')\n",
        "    mask_path = os.path.join(GEE_DIR, base + '_gsw_water.tif')\n",
        "\n",
        "    if not os.path.exists(mask_path):\n",
        "        print(f\"⚠️ No mask for {base}, skipping.\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Processing {base}...\")\n",
        "\n",
        "    img_arr, mask_arr, profile = align_mask_to_image(s2_path, mask_path)\n",
        "\n",
        "    # Simple normalisation (optional – adjust to your UNet)\n",
        "    # Example: Sentinel-2 often scaled by 10,000\n",
        "    img_arr = img_arr / 3000.0  # or 10000.0 depending on stretch\n",
        "\n",
        "    tile_image_and_mask(img_arr, mask_arr, base)\n"
      ],
      "metadata": {
        "id": "LHobeK9BhqD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def align_mask_to_image(img_path, mask_path):\n",
        "    \"\"\"\n",
        "    Returns cleaned:\n",
        "      img_arr: (H, W, 4)\n",
        "      mask_arr: (H, W)\n",
        "    \"\"\"\n",
        "    import rasterio\n",
        "    import numpy as np\n",
        "    from rasterio.warp import reproject, Resampling\n",
        "\n",
        "    # ---- READ IMAGE ----\n",
        "    with rasterio.open(img_path) as src_img:\n",
        "        img = src_img.read().astype('float32')   # (bands, H, W)\n",
        "        img_profile = src_img.profile\n",
        "\n",
        "    # ---- CLEAN IMAGE ----\n",
        "    img = np.nan_to_num(img, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "    # ---- READ MASK ----\n",
        "    with rasterio.open(mask_path) as src_msk:\n",
        "        msk = src_msk.read(1).astype('float32')\n",
        "\n",
        "        # CLEAN MASK\n",
        "        msk = np.nan_to_num(msk, nan=0.0)\n",
        "\n",
        "        # If shapes match, no reprojection needed\n",
        "        if (src_msk.transform == img_profile['transform'] and\n",
        "            src_msk.width == img_profile['width'] and\n",
        "            src_msk.height == img_profile['height']):\n",
        "            mask_resampled = msk\n",
        "        else:\n",
        "            # Reproject to image grid\n",
        "            mask_resampled = np.zeros((img_profile['height'], img_profile['width']),\n",
        "                                      dtype='float32')\n",
        "\n",
        "            reproject(\n",
        "                source=msk,\n",
        "                destination=mask_resampled,\n",
        "                src_transform=src_msk.transform,\n",
        "                src_crs=src_msk.crs,\n",
        "                dst_transform=img_profile['transform'],\n",
        "                dst_crs=img_profile['crs'],\n",
        "                resampling=Resampling.nearest\n",
        "            )\n",
        "\n",
        "    # Move channels last\n",
        "    img_arr = np.transpose(img, (1,2,0))   # (H, W, 4)\n",
        "\n",
        "    # Ensure mask is 0/1\n",
        "    mask_arr = (mask_resampled > 0.5).astype(np.uint8)\n",
        "\n",
        "    return img_arr, mask_arr, img_profile\n"
      ],
      "metadata": {
        "id": "MWLLz57lkVtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for s2_path in s2_files:\n",
        "    base = os.path.basename(s2_path).replace('_s2_4band.tif', '')\n",
        "    mask_path = os.path.join(GEE_DIR, base + '_gsw_water.tif')\n",
        "\n",
        "    if not os.path.exists(mask_path):\n",
        "        print(f\"⚠ No mask for {base}, skipping\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Processing {base}...\")\n",
        "\n",
        "    img_arr, mask_arr, profile = align_mask_to_image(s2_path, mask_path)\n",
        "\n",
        "    # DO NOT NORMALISE HERE\n",
        "    tile_image_and_mask(img_arr, mask_arr, base)\n"
      ],
      "metadata": {
        "id": "7YH82fVmkXur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import glob, os\n",
        "\n",
        "OUT_IMG_DIR = '/content/tiles/images'\n",
        "OUT_MSK_DIR = '/content/tiles/masks'\n",
        "\n",
        "img_tile_path = sorted(glob.glob(os.path.join(OUT_IMG_DIR, 'tharthar_2021*.npy')))[0]\n",
        "msk_tile_path = img_tile_path.replace('/images/', '/masks/')\n",
        "\n",
        "img_tile = np.load(img_tile_path).astype('float32')   # (256,256,4)\n",
        "msk_tile = np.load(msk_tile_path).astype('float32')   # (256,256)\n",
        "\n",
        "# Safety clean (just in case)\n",
        "img_tile = np.nan_to_num(img_tile, nan=0)\n",
        "\n",
        "# Build RGB (raw S2 reflectance)\n",
        "rgb = img_tile[..., [2,1,0]]  # B4,B3,B2\n",
        "\n",
        "# Simple scaling for display\n",
        "rgb_disp = rgb / np.max(rgb)  # auto-scale 0–1\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(rgb_disp)\n",
        "plt.title(\"Raw RGB\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(rgb_disp)\n",
        "plt.imshow(msk_tile, alpha=0.4)\n",
        "plt.title(\"RGB + Mask\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "s5M_38fekhQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob, os\n",
        "import numpy as np\n",
        "\n",
        "OUT_IMG_DIR = '/content/tiles/images'\n",
        "OUT_MSK_DIR = '/content/tiles/masks'\n",
        "\n",
        "tharthar_tiles = sorted(glob.glob(os.path.join(OUT_IMG_DIR, 'tharthar_2021*.npy')))\n",
        "print(\"Found tiles:\", len(tharthar_tiles))\n",
        "\n",
        "best_path = None\n",
        "best_nonzero_frac = -1\n",
        "\n",
        "for path in tharthar_tiles:\n",
        "    tile = np.load(path).astype('float32')\n",
        "    # count non-zero pixels in red band (B4, index 2)\n",
        "    red = tile[..., 2]\n",
        "    nonzero_frac = np.count_nonzero(red) / red.size\n",
        "\n",
        "    if nonzero_frac > best_nonzero_frac:\n",
        "        best_nonzero_frac = nonzero_frac\n",
        "        best_path = path\n",
        "\n",
        "print(\"Best tile:\", best_path)\n",
        "print(\"Best nonzero fraction:\", best_nonzero_frac)\n"
      ],
      "metadata": {
        "id": "4JbUJlktk4IZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img_tile_path = best_path\n",
        "img_tile = np.load(img_tile_path).astype('float32')   # (256,256,4)\n",
        "\n",
        "# extract B4,B3,B2\n",
        "rgb = img_tile[..., [2,1,0]]\n",
        "\n",
        "# simple robust scaling\n",
        "rgb_disp = rgb - np.nanmin(rgb)\n",
        "rgb_disp = rgb_disp / (np.nanmax(rgb_disp) + 1e-6)\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.imshow(rgb_disp)\n",
        "plt.title(f\"Raw RGB – {os.path.basename(img_tile_path)}\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "19g6W-H1k7V-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mask path\n",
        "msk_tile_path = img_tile_path.replace('/images/', '/masks/')\n",
        "msk_tile = np.load(msk_tile_path).astype('float32')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(msk_tile)\n",
        "plt.title(\"Mask\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(rgb_disp)\n",
        "plt.imshow(msk_tile, alpha=0.4)\n",
        "plt.title(\"RGB + Mask\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7IYUjbNOk-vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqJrjauP846-"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTEOTi_qwD0G"
      },
      "source": [
        "'''\n",
        "  Returns an image plot of mask prediction\n",
        "'''\n",
        "\n",
        "def reconstruct_image(model, image, rounded=False):\n",
        "\n",
        "  # Find model prediction\n",
        "  reconstruction = model.predict(image).reshape(image.shape[1], image.shape[2])\n",
        "  # Standardise between 0-1\n",
        "  reconstruction = reconstruction/np.max(reconstruction)\n",
        "\n",
        "  # Round to 0-1, binary pixel-by-pixel classification\n",
        "  if rounded:\n",
        "    reconstruction = np.round(reconstruction)\n",
        "\n",
        "  # Plot reconstructed mask (prediction)\n",
        "  plt.imshow(reconstruction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxUiu0_OxMlo"
      },
      "source": [
        "'''\n",
        "  Returns array of mask prediction, given model and image\n",
        "'''\n",
        "def reconstruct_array(model, image, rounded=False):\n",
        "\n",
        "  # Find model prediction\n",
        "  reconstruction = model.predict(image).reshape(image.shape[1], image.shape[2])\n",
        "\n",
        "  if rounded:\n",
        "    reconstruction = np.round(reconstruction)\n",
        "\n",
        "  return reconstruction # Returns array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prjIhHN-r4o5"
      },
      "source": [
        "'''\n",
        "  Metric functions for evaluation\n",
        "'''\n",
        "\n",
        "def score_eval(model, image, mask): # Gives score of mask vs prediction\n",
        "  if type(image) != list:\n",
        "    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n",
        "    reconstruction = np.round(reconstruction).flatten()\n",
        "\n",
        "    return accuracy_score(mask.flatten(), reconstruction)\n",
        "\n",
        "  else: # If a list of images input, find accuracy for each\n",
        "    scores = []\n",
        "    for i in range(len(image)):\n",
        "      reconstruction = model.predict(image[i].reshape(1, 512, 512, 3))\n",
        "      reconstruction = np.round(reconstruction).flatten()\n",
        "\n",
        "      scores.append(accuracy_score(mask[i].flatten(), reconstruction))\n",
        "\n",
        "    return scores\n",
        "\n",
        "def score_eval2(model, image, mask): # Gives score of mask vs prediction\n",
        "  if type(image) != list:\n",
        "    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n",
        "    reconstruction = np.round(reconstruction).flatten()\n",
        "\n",
        "    return accuracy_score(mask.flatten(), reconstruction)\n",
        "\n",
        "  else: # If a list of images input, find accuracy for each\n",
        "    scores = []\n",
        "    for i in range(len(image)):\n",
        "      reconstruction = model.predict(image[i].reshape(1, 512, 512, 4))\n",
        "      reconstruction = np.round(reconstruction).flatten()\n",
        "\n",
        "      scores.append(accuracy_score(mask[i].flatten(), reconstruction))\n",
        "\n",
        "    return scores\n",
        "\n",
        "def recall_eval(model, image, mask): # Find recall score\n",
        "  if type(image) != list:\n",
        "    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n",
        "    reconstruction = np.round(reconstruction).flatten()\n",
        "\n",
        "    return recall_score(mask.flatten(), reconstruction, average='weighted')\n",
        "\n",
        "  else: # If a list of images input, find accuracy for each\n",
        "    recall = []\n",
        "    for i in range(len(image)):\n",
        "        reconstruction = model.predict(image[i]).reshape(mask[i].shape[1], mask[i].shape[2])\n",
        "        reconstruction = np.round(reconstruction).flatten()\n",
        "\n",
        "        recall.append(recall_score(mask[i].flatten(), reconstruction, average='weighted'))\n",
        "\n",
        "    return recall\n",
        "\n",
        "def precision_eval(model, image, mask): # Find precision score\n",
        "  if type(image) != list:\n",
        "    reconstruction = model.predict(image).reshape(mask.shape[1], mask.shape[2])\n",
        "    reconstruction = np.round(reconstruction).flatten()\n",
        "\n",
        "    return precision_score(mask.flatten(), reconstruction, average='weighted')\n",
        "\n",
        "  else: # If a list of images input, find accuracy for each\n",
        "    precision = []\n",
        "    for i in range(len(image)):\n",
        "        reconstruction = model.predict(image[i]).reshape(mask[i].shape[1], mask[i].shape[2])\n",
        "        reconstruction = np.round(reconstruction).flatten()\n",
        "\n",
        "        precision.append(precision_score(mask[i].flatten(), reconstruction, average='weighted'))\n",
        "\n",
        "    return precision\n",
        "\n",
        "def f1_score_eval(model, image, mask): # Find F1-score\n",
        "    prec = np.mean(precision_eval(model, image, mask))\n",
        "    rec = np.mean(recall_eval(model, image, mask))\n",
        "\n",
        "    if prec + rec == 0:\n",
        "        return 0\n",
        "\n",
        "    return 2 * (prec * rec) / (prec + rec)\n",
        "\n",
        "def f1_score_eval_basic(precision, recall):\n",
        "    prec = np.mean(precision)\n",
        "    rec = np.mean(recall)\n",
        "\n",
        "    if prec + rec == 0:\n",
        "        return 0\n",
        "\n",
        "    return 2 * (prec * rec) / (prec + rec)\n",
        "\n",
        "def produce_mask(image): # Outputs rounded image (binary)\n",
        "  return np.round(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mz6o3t3sveL"
      },
      "source": [
        "# Ingest and Process 4-band Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og7YgBCLct5T"
      },
      "source": [
        "## 4-band Amazon dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "base_dir2 = \"/content/tiles_dataset/\"\n",
        "\n",
        "folders = [\n",
        "    \"Training/image\",\n",
        "    \"Training/label\",\n",
        "    \"Validation/images\",\n",
        "    \"Validation/masks\",\n",
        "    \"Test/image\",\n",
        "    \"Test/mask\",\n",
        "]\n",
        "\n",
        "for f in folders:\n",
        "    os.makedirs(os.path.join(base_dir2, f), exist_ok=True)\n",
        "\n",
        "print(\"Created dataset folders at:\", base_dir2)\n",
        "\n"
      ],
      "metadata": {
        "id": "vwKy9p8JmoMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob, shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "TILES_IMG = \"/content/tiles/images/\"\n",
        "TILES_MSK = \"/content/tiles/masks/\"\n",
        "\n",
        "img_paths = sorted(glob.glob(os.path.join(TILES_IMG, \"*.npy\")))\n",
        "msk_paths = [p.replace(\"/images/\", \"/masks/\") for p in img_paths]\n",
        "\n",
        "print(\"Total tiles found:\", len(img_paths))\n",
        "\n",
        "# 70% train, 15% validation, 15% test\n",
        "X_train, X_tmp, y_train, y_tmp = train_test_split(\n",
        "    img_paths, msk_paths, test_size=0.30, random_state=42\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_tmp, y_tmp, test_size=0.50, random_state=42\n",
        ")\n",
        "\n",
        "def copy_pairs(img_list, msk_list, img_dest, msk_dest):\n",
        "    for img, msk in zip(img_list, msk_list):\n",
        "        shutil.copy(img, img_dest)\n",
        "        shutil.copy(msk, msk_dest)\n",
        "\n",
        "copy_pairs(X_train, y_train, base_dir2 + \"Training/image/\", base_dir2 + \"Training/label/\")\n",
        "copy_pairs(X_val,   y_val,   base_dir2 + \"Validation/images/\", base_dir2 + \"Validation/masks/\")\n",
        "copy_pairs(X_test,  y_test,  base_dir2 + \"Test/image/\",       base_dir2 + \"Test/mask/\")\n",
        "\n",
        "print(\"Tiles copied into Train / Val / Test successfully.\")\n"
      ],
      "metadata": {
        "id": "FARySbo-o5UZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "train_img_dir = base_dir2 + \"Training/image/\"\n",
        "train_msk_dir = base_dir2 + \"Training/label/\"\n",
        "\n",
        "training_images2 = []\n",
        "training_masks2 = []\n",
        "\n",
        "training_files = sorted(os.listdir(train_img_dir))\n",
        "\n",
        "for n in training_files:\n",
        "    img = np.load(train_img_dir + n).astype('float32')       # (256,256,4)\n",
        "    msk = np.load(train_msk_dir + n).astype('float32')       # (256,256)\n",
        "\n",
        "    # normalise image per tile\n",
        "    img = img - np.nanmin(img)\n",
        "    img = img / (np.nanmax(img) + 1e-6)\n",
        "\n",
        "    # binarise + add channel dim\n",
        "    msk = (msk > 0.5).astype('float32')[..., np.newaxis]     # (256,256,1)\n",
        "\n",
        "    training_images2.append(img)\n",
        "    training_masks2.append(msk)\n",
        "\n",
        "training_images2 = np.stack(training_images2)\n",
        "training_masks2  = np.stack(training_masks2)\n",
        "\n",
        "print(\"Train X:\", training_images2.shape)\n",
        "print(\"Train y:\", training_masks2.shape)\n"
      ],
      "metadata": {
        "id": "ClIeg6GxpE-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_img_dir = base_dir2 + \"Validation/images/\"\n",
        "val_msk_dir = base_dir2 + \"Validation/masks/\"\n",
        "\n",
        "validation_images2 = []\n",
        "validation_masks2 = []\n",
        "\n",
        "validation_files = sorted(os.listdir(val_img_dir))\n",
        "\n",
        "for n in validation_files:\n",
        "    img = np.load(val_img_dir + n).astype('float32')\n",
        "    msk = np.load(val_msk_dir + n).astype('float32')\n",
        "\n",
        "    img = img - np.nanmin(img)\n",
        "    img = img / (np.nanmax(img) + 1e-6)\n",
        "\n",
        "    msk = (msk > 0.5).astype('float32')[..., np.newaxis]\n",
        "\n",
        "    validation_images2.append(img)\n",
        "    validation_masks2.append(msk)\n",
        "\n",
        "validation_images2 = np.stack(validation_images2)\n",
        "validation_masks2  = np.stack(validation_masks2)\n",
        "\n",
        "print(\"Val X:\", validation_images2.shape)\n",
        "print(\"Val y:\", validation_masks2.shape)\n"
      ],
      "metadata": {
        "id": "JNHa9BpIpH2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_img_dir = base_dir2 + \"Test/image/\"\n",
        "test_msk_dir = base_dir2 + \"Test/mask/\"\n",
        "\n",
        "test_images2 = []\n",
        "test_masks2 = []\n",
        "\n",
        "test_files = sorted(os.listdir(test_img_dir))\n",
        "\n",
        "for n in test_files:\n",
        "    img = np.load(test_img_dir + n).astype('float32')\n",
        "    msk = np.load(test_msk_dir + n).astype('float32')\n",
        "\n",
        "    img = img - np.nanmin(img)\n",
        "    img = img / (np.nanmax(img) + 1e-6)\n",
        "\n",
        "    msk = (msk > 0.5).astype('float32')[..., np.newaxis]\n",
        "\n",
        "    test_images2.append(img)\n",
        "    test_masks2.append(msk)\n",
        "\n",
        "test_images2 = np.stack(test_images2)\n",
        "test_masks2  = np.stack(test_masks2)\n",
        "\n",
        "print(\"Test X:\", test_images2.shape)\n",
        "print(\"Test y:\", test_masks2.shape)\n"
      ],
      "metadata": {
        "id": "GtnIEHl-pKhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== \"OG-style\" preprocessing ADAPTED TO 256×256 ====\n",
        "\n",
        "# Train\n",
        "for i in range(len(training_images2)):\n",
        "    # same as OG: ensure float32\n",
        "    training_images2[i] = training_images2[i].astype('float32')\n",
        "    # OG did .T because original was (4, H, W).\n",
        "    # Your images are already (H, W, C), so DO NOT transpose.\n",
        "\n",
        "for i in range(len(training_masks2)):\n",
        "    training_masks2[i] = training_masks2[i].astype('float32')\n",
        "    # OG did reshape(1,512,512,1).T; here masks are already (256,256,1), so no transpose/reshape needed.\n",
        "\n",
        "# If you have validation / test arrays loaded similarly:\n",
        "validation_images2  = np.stack(validation_images2)   # (N_val,256,256,4)\n",
        "validation_masks2   = np.stack(validation_masks2)    # (N_val,256,256,1)\n",
        "test_images2        = np.stack(test_images2)         # (N_test,256,256,4)\n",
        "test_masks2         = np.stack(test_masks2)          # (N_test,256,256,1)\n",
        "\n",
        "for i in range(len(validation_images2)):\n",
        "    validation_images2[i] = validation_images2[i].astype('float32')\n",
        "\n",
        "for i in range(len(validation_masks2)):\n",
        "    validation_masks2[i] = validation_masks2[i].astype('float32')\n",
        "\n",
        "for i in range(len(test_images2)):\n",
        "    test_images2[i] = test_images2[i].astype('float32')\n",
        "\n",
        "for i in range(len(test_masks2)):\n",
        "    test_masks2[i] = test_masks2[i].astype('float32')\n",
        "\n",
        "# Final reshapes – OG had (-1,512,512,4); we keep same idea but with 256:\n",
        "training_images2   = training_images2.reshape(-1, 256, 256, 4)\n",
        "training_masks2    = training_masks2.reshape(-1, 256, 256, 1)\n",
        "validation_images2 = validation_images2.reshape(-1, 256, 256, 4)\n",
        "validation_masks2  = validation_masks2.reshape(-1, 256, 256, 1)\n",
        "test_images2       = test_images2.reshape(-1, 256, 256, 4)\n",
        "test_masks2        = test_masks2.reshape(-1, 256, 256, 1)\n",
        "\n",
        "print(\"Train X final:\", training_images2.shape)\n",
        "print(\"Train y final:\", training_masks2.shape)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 2. FUNCTION TO SHOW A TRAIN IMAGE AS RGB (B4,B3,B2)\n",
        "# -----------------------------\n",
        "\n",
        "def show_train_example(idx):\n",
        "    img  = training_images2[idx]        # (256,256,4)\n",
        "    mask = training_masks2[idx][...,0]  # (256,256)\n",
        "\n",
        "    # Build RGB (Sentinel-2 True Colour)\n",
        "    # B4 = Red (index 2)\n",
        "    # B3 = Green (index 1)\n",
        "    # B2 = Blue (index 0)\n",
        "    rgb = img[..., [2,1,0]]\n",
        "\n",
        "    # Normalise for display\n",
        "    rgb_disp = (rgb - rgb.min()) / (rgb.max() + 1e-6)\n",
        "\n",
        "    plt.figure(figsize=(12,5))\n",
        "\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.imshow(rgb_disp)\n",
        "    plt.title(f\"Training Image #{idx} (RGB)\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.imshow(rgb_disp)\n",
        "    plt.imshow(mask, alpha=0.4, cmap=\"Blues\")\n",
        "    plt.title(\"RGB + Mask Overlay\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 3. SHOW EXAMPLE IMAGE\n",
        "# -----------------------------\n",
        "\n",
        "show_train_example(20)   # change index freely\n"
      ],
      "metadata": {
        "id": "9LAoJy6yI1Sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCp3t2JSyQ9O"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDxQ-_Ao7NBG"
      },
      "source": [
        "## U-Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--tUx5qNR0Q4"
      },
      "source": [
        "'''\n",
        "  Convolutional block with set parameters and activation layer after\n",
        "'''\n",
        "\n",
        "def convBlock(input, filters, kernel, kernel_init='he_normal', act='relu', transpose=False):\n",
        "  if transpose == False:\n",
        "    #conv = ZeroPadding2D((1,1))(input)\n",
        "    conv = Conv2D(filters, kernel, padding = 'same', kernel_initializer = kernel_init)(input)\n",
        "  else:\n",
        "    #conv = ZeroPadding2D((1,1))(input)\n",
        "    conv = Conv2DTranspose(filters, kernel, padding = 'same', kernel_initializer = kernel_init)(input)\n",
        "\n",
        "  conv = Activation(act)(conv)\n",
        "  return conv\n",
        "\n",
        "'''\n",
        "  U-Net model\n",
        "'''\n",
        "\n",
        "def UNet(trained_weights = None, input_size = (512,512,3), drop_rate = 0.25, lr=0.0001):\n",
        "\n",
        "    ## Can add pretrained weights by specifying 'trained_weights'\n",
        "\n",
        "    # Input layer\n",
        "    inputs = Input(input_size)\n",
        "\n",
        "    ## Contraction phase\n",
        "    conv1 = convBlock(inputs, 64, 3)\n",
        "    conv1 = convBlock(conv1, 64, 3)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = convBlock(pool1, 128, 3)\n",
        "    conv2 = convBlock(conv2, 128, 3)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    #drop2 = Dropout(drop_rate)(pool2)\n",
        "\n",
        "    conv3 = convBlock(pool2, 256, 3)\n",
        "    conv3 = convBlock(conv3, 256, 3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "    #drop3 = Dropout(drop_rate)(pool3)\n",
        "\n",
        "    conv4 = convBlock(pool3, 512, 3)\n",
        "    conv4 = convBlock(conv4, 512, 3)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "    #drop4 = Dropout(drop_rate)(pool4)\n",
        "\n",
        "    conv5 = convBlock(pool4, 1024, 3)\n",
        "    conv5 = convBlock(conv5, 1024, 3)\n",
        "\n",
        "    ## Expansion phase\n",
        "    up6 = (Conv2DTranspose(512, kernel_size=2, strides=2, kernel_initializer='he_normal')(conv5))\n",
        "    merge6 = concatenate([conv4,up6])\n",
        "    conv6 = convBlock(merge6, 512, 3)\n",
        "    conv6 = convBlock(conv6, 512, 3)\n",
        "    #conv6 = Dropout(drop_rate)(conv6)\n",
        "\n",
        "    up7 = (Conv2DTranspose(256, kernel_size=2, strides=2, kernel_initializer='he_normal')(conv6))\n",
        "    merge7 = concatenate([conv3,up7])\n",
        "    conv7 = convBlock(merge7, 256, 3)\n",
        "    conv7 = convBlock(conv7, 256, 3)\n",
        "    #conv7 = Dropout(drop_rate)(conv7)\n",
        "\n",
        "    up8 = (Conv2DTranspose(128, kernel_size=2, strides=2, kernel_initializer='he_normal')(conv7))\n",
        "    merge8 = concatenate([conv2,up8])\n",
        "    conv8 = convBlock(merge8, 128, 3)\n",
        "    conv8 = convBlock(conv8, 128, 3)\n",
        "    #conv8 = Dropout(drop_rate)(conv8)\n",
        "\n",
        "    up9 = (Conv2DTranspose(64, kernel_size=2, strides=2, kernel_initializer='he_normal')(conv8))\n",
        "    merge9 = concatenate([conv1,up9])\n",
        "    conv9 = convBlock(merge9, 64, 3)\n",
        "    conv9 = convBlock(conv9, 64, 3)\n",
        "\n",
        "    # Output layer\n",
        "    conv10 = convBlock(conv9, 1, 1, act='sigmoid')\n",
        "\n",
        "    model = Model(inputs, conv10)\n",
        "\n",
        "    model.compile(optimizer = Adam(learning_rate = lr), loss = 'binary_crossentropy', metrics = ['accuracy', 'mse'])\n",
        "\n",
        "    if trained_weights != None:\n",
        "    \tmodel.load_weights(trained_weights)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD6fODjlcLdG"
      },
      "source": [
        "# Print model layers and number of parameters\n",
        "UNet().summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJqa5zAobgr2"
      },
      "source": [
        "## Attention U-Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3uY93JpT_OZ"
      },
      "source": [
        "'''\n",
        "  Convolutional block with two conv layers and two activation layers\n",
        "'''\n",
        "\n",
        "def convBlock2(input, filters, kernel, kernel_init='he_normal', act='relu', transpose=False):\n",
        "  if transpose == False:\n",
        "    conv = Conv2D(filters, kernel, padding = 'same', kernel_initializer = kernel_init)(input)\n",
        "    conv = Activation(act)(conv)\n",
        "    conv = Conv2D(filters, kernel, padding = 'same', kernel_initializer = kernel_init)(conv)\n",
        "    conv = Activation(act)(conv)\n",
        "  else:\n",
        "    conv = Conv2DTranspose(filters, kernel, padding = 'same', kernel_initializer = kernel_init)(input)\n",
        "    conv = Activation(act)(conv)\n",
        "    conv = Conv2DTranspose(filters, kernel, padding = 'same', kernel_initializer = kernel_init)(conv)\n",
        "    conv = Activation(act)(conv)\n",
        "\n",
        "  return conv\n",
        "\n",
        "'''\n",
        "  Attention block/mechanism\n",
        "'''\n",
        "def attention_block(x, gating, inter_shape, drop_rate=0.25):\n",
        "\n",
        "    # Find shape of inputs\n",
        "    shape_x = K.int_shape(x)\n",
        "    shape_g = K.int_shape(gating)\n",
        "\n",
        "    ## Process x vector and gating signal\n",
        "    # x vector input and processing\n",
        "    theta_x = Conv2D(inter_shape, kernel_size = 1, strides = 1, padding='same', kernel_initializer='he_normal', activation=None)(x)\n",
        "    theta_x = MaxPooling2D((2,2))(theta_x)\n",
        "    shape_theta_x = K.int_shape(theta_x)\n",
        "\n",
        "    # gating signal \"\"\n",
        "    phi_g = Conv2D(inter_shape, kernel_size = 1, strides = 1, padding='same', kernel_initializer='he_normal', activation=None)(gating)\n",
        "    shape_phi_g = K.int_shape(phi_g)\n",
        "\n",
        "    # Add components\n",
        "    concat_xg = add([phi_g, theta_x])\n",
        "    act_xg = Activation('relu')(concat_xg)\n",
        "\n",
        "    # Apply convolution\n",
        "    psi = Conv2D(1, kernel_size = 1, strides = 1, padding='same', kernel_initializer='he_normal', activation=None)(act_xg)\n",
        "\n",
        "    # Apply sigmoid activation\n",
        "    sigmoid_xg = Activation('sigmoid')(psi)\n",
        "    shape_sigmoid = K.int_shape(sigmoid_xg)\n",
        "\n",
        "    # UpSample and resample to correct size\n",
        "    upsample_psi = UpSampling2D(\n",
        "        interpolation='bilinear',\n",
        "        size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2])\n",
        "    )(sigmoid_xg)\n",
        "\n",
        "    # No tf.broadcast_to – Keras will broadcast automatically in multiply()\n",
        "    y = multiply([upsample_psi, x])\n",
        "\n",
        "\n",
        "    return y\n",
        "\n",
        "\n",
        "'''\n",
        "  Attention U-Net model\n",
        "'''\n",
        "\n",
        "def UNetAM(trained_weights = None, input_size = (512,512,3), drop_rate = 0.25, lr=0.0001, filter_base=16):\n",
        "\n",
        "    ## Can add pretrained weights by specifying 'trained_weights'\n",
        "\n",
        "    # Input layer\n",
        "    inputs = Input(input_size, batch_size=1)\n",
        "\n",
        "    ## Contraction phase\n",
        "    conv = convBlock2(inputs, filter_base, 3)\n",
        "    #conv0 = Dropout(drop_rate)(conv0)\n",
        "\n",
        "    conv0 = MaxPooling2D(pool_size=(2, 2))(conv)\n",
        "    conv0 = convBlock2(conv0, 2 * filter_base, 3)\n",
        "\n",
        "    pool0 = MaxPooling2D(pool_size=(2, 2))(conv0)\n",
        "    conv1 = convBlock2(pool0, 4 * filter_base, 3)\n",
        "    #conv1 = Dropout(drop_rate)(conv1)\n",
        "\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    conv2 = convBlock2(pool1, 8 * filter_base, 3)\n",
        "    #conv2 = Dropout(drop_rate)(conv2)\n",
        "\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    conv3 = convBlock2(pool2, 16 * filter_base, 3)\n",
        "    #conv3 = Dropout(drop_rate)(conv3)\n",
        "\n",
        "    ## Expansion phase\n",
        "    up4 = (Conv2DTranspose(8 * filter_base, kernel_size=2, strides=2, kernel_initializer='he_normal')(conv3))\n",
        "    merge4 = attention_block(conv2, conv3, 8 * filter_base, drop_rate) # Attention gate\n",
        "    conv4 = concatenate([up4, merge4])\n",
        "    conv4 = convBlock2(conv4, 8 * filter_base, 3)\n",
        "\n",
        "    up5 = (Conv2DTranspose(4 * filter_base, kernel_size=2, strides=2, kernel_initializer='he_normal')(conv4))\n",
        "    merge5 = attention_block(conv1, conv4, 4 * filter_base, drop_rate) # Attention gate\n",
        "    conv5 = concatenate([up5, merge5])\n",
        "    conv5 = convBlock2(conv5, 4 * filter_base, 3)\n",
        "\n",
        "    up6 = (Conv2DTranspose(2 * filter_base, kernel_size=2, strides=2, kernel_initializer='he_normal')(conv5))\n",
        "    merge6 = attention_block(conv0, conv5, 2 * filter_base, drop_rate) # Attention gate\n",
        "    conv6 = concatenate([up6, merge6])\n",
        "    conv6 = convBlock2(conv6, 2 * filter_base, 3)\n",
        "\n",
        "    up7 = (Conv2DTranspose(1 * filter_base, kernel_size=2, strides=2, kernel_initializer='he_normal')(conv6))\n",
        "    merge7 = attention_block(conv, conv6, 1 * filter_base, drop_rate) # Attention gate\n",
        "    conv7 = concatenate([up7, merge7])\n",
        "    conv7 = concatenate([up7, conv])\n",
        "    conv7 = convBlock2(conv7, 1 * filter_base, 3)\n",
        "\n",
        "    ## Output layer\n",
        "    out = convBlock(conv7, 1, 1, act='sigmoid')\n",
        "\n",
        "    model = Model(inputs, out)\n",
        "\n",
        "    model.compile(optimizer = Adam(learning_rate = lr), loss = binary_crossentropy, metrics = ['accuracy', 'mse'])\n",
        "\n",
        "    if trained_weights != None:\n",
        "    \tmodel.load_weights(trained_weights)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2fZD22p5xsF"
      },
      "source": [
        "# Print model layers and number of parameters\n",
        "UNetAM().summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5KsYCx34JdT"
      },
      "source": [
        "# Train on 4-band data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdPUGj0iCqX9"
      },
      "source": [
        "## Train on 4-band Amazon data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGZXjnnfiFU6"
      },
      "source": [
        "### U-Net"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Build 4-band UNet for 256×256 tiles\n",
        "model_unet_4band = UNet(input_size=(256,256,4), lr=0.0001)\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((training_images2, training_masks2))\n",
        "train_ds = train_ds.shuffle(2000).batch(BATCH_SIZE).repeat()\n",
        "\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((validation_images2, validation_masks2))\n",
        "val_ds = val_ds.batch(BATCH_SIZE)\n",
        "\n",
        "\n",
        "# Save best model by validation accuracy\n",
        "save_model_4band = ModelCheckpoint(\n",
        "    'unet-4band-lakes.keras',\n",
        "    monitor='val_accuracy',\n",
        "    verbose=1,\n",
        "    save_best_only=True\n",
        ")\n",
        "# Train directly on NumPy arrays\n",
        "history_4band = model_unet_4band.fit(\n",
        "    train_ds,\n",
        "    steps_per_epoch=100,          # same style as original code\n",
        "    epochs=30,                    # you can increase to 40–60\n",
        "    validation_data=val_ds,\n",
        "    callbacks=[save_model_4band],\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCmLPfpwsFSE",
        "outputId": "080b8ff6-ab79-492e-bdcc-cad4e4377138"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.7269 - loss: 0.6100 - mse: 0.2077\n",
            "Epoch 1: val_accuracy improved from -inf to 0.77833, saving model to unet-4band-lakes.keras\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 397ms/step - accuracy: 0.7271 - loss: 0.6097 - mse: 0.2075 - val_accuracy: 0.7783 - val_loss: 0.5277 - val_mse: 0.1750\n",
            "Epoch 2/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.7530 - loss: 0.5585 - mse: 0.1875\n",
            "Epoch 2: val_accuracy improved from 0.77833 to 0.78000, saving model to unet-4band-lakes.keras\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 263ms/step - accuracy: 0.7531 - loss: 0.5584 - mse: 0.1874 - val_accuracy: 0.7800 - val_loss: 0.5003 - val_mse: 0.1657\n",
            "Epoch 3/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 382ms/step - accuracy: 0.7579 - loss: 0.5107 - mse: 0.1722\n",
            "Epoch 3: val_accuracy did not improve from 0.78000\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 425ms/step - accuracy: 0.7580 - loss: 0.5106 - mse: 0.1721 - val_accuracy: 0.7756 - val_loss: 0.5051 - val_mse: 0.1697\n",
            "Epoch 4/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.7613 - loss: 0.4984 - mse: 0.1674\n",
            "Epoch 4: val_accuracy did not improve from 0.78000\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 246ms/step - accuracy: 0.7614 - loss: 0.4984 - mse: 0.1674 - val_accuracy: 0.7613 - val_loss: 0.5021 - val_mse: 0.1679\n",
            "Epoch 5/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.7840 - loss: 0.4855 - mse: 0.1615\n",
            "Epoch 5: val_accuracy did not improve from 0.78000\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 246ms/step - accuracy: 0.7839 - loss: 0.4855 - mse: 0.1615 - val_accuracy: 0.7769 - val_loss: 0.4703 - val_mse: 0.1556\n",
            "Epoch 6/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.7641 - loss: 0.4806 - mse: 0.1604\n",
            "Epoch 6: val_accuracy did not improve from 0.78000\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 246ms/step - accuracy: 0.7643 - loss: 0.4804 - mse: 0.1603 - val_accuracy: 0.7747 - val_loss: 0.4791 - val_mse: 0.1581\n",
            "Epoch 7/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.7671 - loss: 0.4903 - mse: 0.1635\n",
            "Epoch 7: val_accuracy did not improve from 0.78000\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 245ms/step - accuracy: 0.7672 - loss: 0.4902 - mse: 0.1634 - val_accuracy: 0.7771 - val_loss: 0.4560 - val_mse: 0.1496\n",
            "Epoch 8/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.7834 - loss: 0.4545 - mse: 0.1500\n",
            "Epoch 8: val_accuracy did not improve from 0.78000\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 246ms/step - accuracy: 0.7833 - loss: 0.4547 - mse: 0.1500 - val_accuracy: 0.7698 - val_loss: 0.4578 - val_mse: 0.1509\n",
            "Epoch 9/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.7869 - loss: 0.4692 - mse: 0.1548\n",
            "Epoch 9: val_accuracy improved from 0.78000 to 0.80472, saving model to unet-4band-lakes.keras\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 259ms/step - accuracy: 0.7872 - loss: 0.4689 - mse: 0.1547 - val_accuracy: 0.8047 - val_loss: 0.4424 - val_mse: 0.1436\n",
            "Epoch 10/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.7899 - loss: 0.4498 - mse: 0.1477\n",
            "Epoch 10: val_accuracy did not improve from 0.80472\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 246ms/step - accuracy: 0.7899 - loss: 0.4497 - mse: 0.1476 - val_accuracy: 0.7960 - val_loss: 0.4389 - val_mse: 0.1424\n",
            "Epoch 11/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.7920 - loss: 0.4345 - mse: 0.1424\n",
            "Epoch 11: val_accuracy improved from 0.80472 to 0.81194, saving model to unet-4band-lakes.keras\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 261ms/step - accuracy: 0.7920 - loss: 0.4346 - mse: 0.1424 - val_accuracy: 0.8119 - val_loss: 0.4484 - val_mse: 0.1455\n",
            "Epoch 12/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.7992 - loss: 0.4397 - mse: 0.1430\n",
            "Epoch 12: val_accuracy did not improve from 0.81194\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 245ms/step - accuracy: 0.7992 - loss: 0.4396 - mse: 0.1430 - val_accuracy: 0.7960 - val_loss: 0.4444 - val_mse: 0.1439\n",
            "Epoch 13/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.7998 - loss: 0.4263 - mse: 0.1382\n",
            "Epoch 13: val_accuracy did not improve from 0.81194\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 246ms/step - accuracy: 0.7998 - loss: 0.4263 - mse: 0.1382 - val_accuracy: 0.8048 - val_loss: 0.4395 - val_mse: 0.1429\n",
            "Epoch 14/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.8181 - loss: 0.4281 - mse: 0.1341\n",
            "Epoch 14: val_accuracy did not improve from 0.81194\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 245ms/step - accuracy: 0.8180 - loss: 0.4280 - mse: 0.1341 - val_accuracy: 0.7933 - val_loss: 0.4330 - val_mse: 0.1406\n",
            "Epoch 15/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.7955 - loss: 0.4305 - mse: 0.1404\n",
            "Epoch 15: val_accuracy improved from 0.81194 to 0.81895, saving model to unet-4band-lakes.keras\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 259ms/step - accuracy: 0.7954 - loss: 0.4306 - mse: 0.1405 - val_accuracy: 0.8189 - val_loss: 0.4140 - val_mse: 0.1325\n",
            "Epoch 16/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.8106 - loss: 0.4228 - mse: 0.1373\n",
            "Epoch 16: val_accuracy did not improve from 0.81895\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 244ms/step - accuracy: 0.8106 - loss: 0.4227 - mse: 0.1373 - val_accuracy: 0.8185 - val_loss: 0.4124 - val_mse: 0.1323\n",
            "Epoch 17/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.8216 - loss: 0.4109 - mse: 0.1325\n",
            "Epoch 17: val_accuracy improved from 0.81895 to 0.82357, saving model to unet-4band-lakes.keras\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 260ms/step - accuracy: 0.8216 - loss: 0.4109 - mse: 0.1325 - val_accuracy: 0.8236 - val_loss: 0.4160 - val_mse: 0.1331\n",
            "Epoch 18/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.8182 - loss: 0.4069 - mse: 0.1307\n",
            "Epoch 18: val_accuracy did not improve from 0.82357\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 244ms/step - accuracy: 0.8181 - loss: 0.4070 - mse: 0.1308 - val_accuracy: 0.7912 - val_loss: 0.4439 - val_mse: 0.1468\n",
            "Epoch 19/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.8211 - loss: 0.3897 - mse: 0.1259\n",
            "Epoch 19: val_accuracy did not improve from 0.82357\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 245ms/step - accuracy: 0.8211 - loss: 0.3897 - mse: 0.1259 - val_accuracy: 0.8137 - val_loss: 0.4171 - val_mse: 0.1348\n",
            "Epoch 20/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.7989 - loss: 0.4299 - mse: 0.1393\n",
            "Epoch 20: val_accuracy improved from 0.82357 to 0.82703, saving model to unet-4band-lakes.keras\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 259ms/step - accuracy: 0.7990 - loss: 0.4298 - mse: 0.1393 - val_accuracy: 0.8270 - val_loss: 0.4040 - val_mse: 0.1289\n",
            "Epoch 21/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.8257 - loss: 0.3852 - mse: 0.1238\n",
            "Epoch 21: val_accuracy did not improve from 0.82703\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 244ms/step - accuracy: 0.8258 - loss: 0.3851 - mse: 0.1238 - val_accuracy: 0.7725 - val_loss: 0.5068 - val_mse: 0.1694\n",
            "Epoch 22/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.8028 - loss: 0.4158 - mse: 0.1369\n",
            "Epoch 22: val_accuracy improved from 0.82703 to 0.83464, saving model to unet-4band-lakes.keras\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 260ms/step - accuracy: 0.8029 - loss: 0.4159 - mse: 0.1369 - val_accuracy: 0.8346 - val_loss: 0.3968 - val_mse: 0.1260\n",
            "Epoch 23/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.8196 - loss: 0.3979 - mse: 0.1287\n",
            "Epoch 23: val_accuracy did not improve from 0.83464\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 244ms/step - accuracy: 0.8196 - loss: 0.3980 - mse: 0.1288 - val_accuracy: 0.8317 - val_loss: 0.3969 - val_mse: 0.1265\n",
            "Epoch 24/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.8312 - loss: 0.3887 - mse: 0.1243\n",
            "Epoch 24: val_accuracy did not improve from 0.83464\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 245ms/step - accuracy: 0.8312 - loss: 0.3887 - mse: 0.1243 - val_accuracy: 0.7924 - val_loss: 0.4364 - val_mse: 0.1445\n",
            "Epoch 25/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.7957 - loss: 0.4339 - mse: 0.1425\n",
            "Epoch 25: val_accuracy did not improve from 0.83464\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 244ms/step - accuracy: 0.7957 - loss: 0.4339 - mse: 0.1425 - val_accuracy: 0.7886 - val_loss: 0.4321 - val_mse: 0.1406\n",
            "Epoch 26/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.8039 - loss: 0.4070 - mse: 0.1333\n",
            "Epoch 26: val_accuracy did not improve from 0.83464\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 245ms/step - accuracy: 0.8040 - loss: 0.4071 - mse: 0.1333 - val_accuracy: 0.8044 - val_loss: 0.4165 - val_mse: 0.1354\n",
            "Epoch 27/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.8170 - loss: 0.3951 - mse: 0.1278\n",
            "Epoch 27: val_accuracy did not improve from 0.83464\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 244ms/step - accuracy: 0.8171 - loss: 0.3950 - mse: 0.1278 - val_accuracy: 0.7985 - val_loss: 0.4433 - val_mse: 0.1433\n",
            "Epoch 28/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.8367 - loss: 0.3888 - mse: 0.1228\n",
            "Epoch 28: val_accuracy did not improve from 0.83464\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 245ms/step - accuracy: 0.8365 - loss: 0.3889 - mse: 0.1229 - val_accuracy: 0.8242 - val_loss: 0.4032 - val_mse: 0.1287\n",
            "Epoch 29/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.8353 - loss: 0.3780 - mse: 0.1212\n",
            "Epoch 29: val_accuracy did not improve from 0.83464\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 244ms/step - accuracy: 0.8354 - loss: 0.3780 - mse: 0.1212 - val_accuracy: 0.8135 - val_loss: 0.4052 - val_mse: 0.1320\n",
            "Epoch 30/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.8261 - loss: 0.3941 - mse: 0.1254\n",
            "Epoch 30: val_accuracy did not improve from 0.83464\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 244ms/step - accuracy: 0.8261 - loss: 0.3941 - mse: 0.1254 - val_accuracy: 0.8271 - val_loss: 0.4051 - val_mse: 0.1286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5Sf1pmCDI-c"
      },
      "source": [
        "# Save model history\n",
        "np.save('unet-4d-history.npy',model_unet_4band.history.history)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM86ZMtryOi2"
      },
      "source": [
        "# Copy models to drive\n",
        "!cp unet-4band-lakes.keras /content/drive/MyDrive/Diss/\n",
        "!cp unet-4d-history.npy drive/MyDrive/Diss/"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HICnX_kEQDhR"
      },
      "source": [
        "### Attention U-Net"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((training_images2, training_masks2))\n",
        "train_ds = train_ds.shuffle(2000).batch(BATCH_SIZE).repeat()\n",
        "\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((validation_images2, validation_masks2))\n",
        "val_ds = val_ds.batch(BATCH_SIZE)\n",
        "\n",
        "# ---------------------------\n",
        "# 2) Build Attention U-Net (4-band, 256x256)\n",
        "#    UNetAM must already be defined in your notebook\n",
        "# ---------------------------\n",
        "model_attention_unet_4band = UNetAM(\n",
        "    input_size=(256, 256, 4),\n",
        "    filter_base=16,\n",
        "    lr=0.0005\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# 3) Checkpoint callback (attention model)\n",
        "# ---------------------------\n",
        "save_model_4band_attention = ModelCheckpoint(\n",
        "    'unet-attention-4band-lakes.keras',\n",
        "    monitor='val_accuracy',\n",
        "    verbose=1,\n",
        "    save_best_only=True\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# 4) Train with generator (same style as original Amazon code)\n",
        "# ---------------------------\n",
        "history_attention_4band = model_attention_unet_4band.fit(\n",
        "    train_ds,\n",
        "    steps_per_epoch=100,       # like your original: more iterations per epoch\n",
        "    epochs=60,                 # same as your old attention UNet\n",
        "    validation_data=val_ds,\n",
        "    callbacks=[save_model_4band_attention],\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "tVjAOV6ztmeu",
        "outputId": "7a10e19b-c8ed-43d8-bdd8-070925f3ed07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7142 - loss: 0.6387 - mse: 0.2216\n",
            "Epoch 1: val_accuracy improved from -inf to 0.77295, saving model to unet-attention-4band-lakes.keras\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 106ms/step - accuracy: 0.7146 - loss: 0.6382 - mse: 0.2213 - val_accuracy: 0.7730 - val_loss: 0.5454 - val_mse: 0.1804\n",
            "Epoch 2/60\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7675 - loss: 0.5121 - mse: 0.1690\n",
            "Epoch 2: val_accuracy improved from 0.77295 to 0.77348, saving model to unet-attention-4band-lakes.keras\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.7675 - loss: 0.5120 - mse: 0.1689 - val_accuracy: 0.7735 - val_loss: 0.4969 - val_mse: 0.1655\n",
            "Epoch 3/60\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.7537 - loss: 0.5048 - mse: 0.1709\n",
            "Epoch 3: val_accuracy did not improve from 0.77348\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 160ms/step - accuracy: 0.7539 - loss: 0.5045 - mse: 0.1708 - val_accuracy: 0.7596 - val_loss: 0.5221 - val_mse: 0.1774\n",
            "Epoch 4/60\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7625 - loss: 0.5035 - mse: 0.1701\n",
            "Epoch 4: val_accuracy did not improve from 0.77348\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.7626 - loss: 0.5030 - mse: 0.1700 - val_accuracy: 0.7234 - val_loss: 0.5443 - val_mse: 0.1841\n",
            "Epoch 5/60\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7627 - loss: 0.4794 - mse: 0.1613\n",
            "Epoch 5: val_accuracy did not improve from 0.77348\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.7629 - loss: 0.4792 - mse: 0.1613 - val_accuracy: 0.7638 - val_loss: 0.4677 - val_mse: 0.1549\n",
            "Epoch 6/60\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7503 - loss: 0.4784 - mse: 0.1607\n",
            "Epoch 6: val_accuracy improved from 0.77348 to 0.77445, saving model to unet-attention-4band-lakes.keras\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.7505 - loss: 0.4782 - mse: 0.1607 - val_accuracy: 0.7744 - val_loss: 0.4663 - val_mse: 0.1524\n",
            "Epoch 7/60\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7765 - loss: 0.4781 - mse: 0.1590\n",
            "Epoch 7: val_accuracy did not improve from 0.77445\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.7765 - loss: 0.4781 - mse: 0.1590 - val_accuracy: 0.7704 - val_loss: 0.5198 - val_mse: 0.1724\n",
            "Epoch 8/60\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7774 - loss: 0.4859 - mse: 0.1598\n",
            "Epoch 8: val_accuracy improved from 0.77445 to 0.78008, saving model to unet-attention-4band-lakes.keras\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.7771 - loss: 0.4859 - mse: 0.1598 - val_accuracy: 0.7801 - val_loss: 0.4734 - val_mse: 0.1541\n",
            "Epoch 9/60\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7721 - loss: 0.4794 - mse: 0.1583\n",
            "Epoch 9: val_accuracy improved from 0.78008 to 0.78008, saving model to unet-attention-4band-lakes.keras\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.7719 - loss: 0.4793 - mse: 0.1583 - val_accuracy: 0.7801 - val_loss: 0.4538 - val_mse: 0.1480\n",
            "Epoch 10/60\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8018 - loss: 0.4380 - mse: 0.1428\n",
            "Epoch 10: val_accuracy did not improve from 0.78008\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.8016 - loss: 0.4384 - mse: 0.1430 - val_accuracy: 0.7544 - val_loss: 0.4764 - val_mse: 0.1598\n",
            "Epoch 11/60\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7779 - loss: 0.4513 - mse: 0.1487\n",
            "Epoch 11: val_accuracy did not improve from 0.78008\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.7780 - loss: 0.4512 - mse: 0.1487 - val_accuracy: 0.7649 - val_loss: 0.4905 - val_mse: 0.1633\n",
            "Epoch 12/60\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7884 - loss: 0.4373 - mse: 0.1448\n",
            "Epoch 12: val_accuracy improved from 0.78008 to 0.79333, saving model to unet-attention-4band-lakes.keras\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.7884 - loss: 0.4374 - mse: 0.1449 - val_accuracy: 0.7933 - val_loss: 0.4529 - val_mse: 0.1482\n",
            "Epoch 13/60\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7789 - loss: 0.4495 - mse: 0.1490\n",
            "Epoch 13: val_accuracy did not improve from 0.79333\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.7789 - loss: 0.4495 - mse: 0.1489 - val_accuracy: 0.7664 - val_loss: 0.4904 - val_mse: 0.1639\n",
            "Epoch 14/60\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7926 - loss: 0.4295 - mse: 0.1411\n",
            "Epoch 14: val_accuracy improved from 0.79333 to 0.79831, saving model to unet-attention-4band-lakes.keras\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.7925 - loss: 0.4298 - mse: 0.1412 - val_accuracy: 0.7983 - val_loss: 0.4416 - val_mse: 0.1439\n",
            "Epoch 15/60\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7794 - loss: 0.4442 - mse: 0.1471\n",
            "Epoch 15: val_accuracy did not improve from 0.79831\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.7792 - loss: 0.4444 - mse: 0.1472 - val_accuracy: 0.7693 - val_loss: 0.4491 - val_mse: 0.1479\n",
            "Epoch 16/60\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8060 - loss: 0.4206 - mse: 0.1373\n",
            "Epoch 16: val_accuracy did not improve from 0.79831\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.8058 - loss: 0.4208 - mse: 0.1374 - val_accuracy: 0.7664 - val_loss: 0.4498 - val_mse: 0.1489\n",
            "Epoch 17/60\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7806 - loss: 0.4442 - mse: 0.1470\n",
            "Epoch 17: val_accuracy did not improve from 0.79831\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.7805 - loss: 0.4444 - mse: 0.1471 - val_accuracy: 0.7810 - val_loss: 0.4485 - val_mse: 0.1475\n",
            "Epoch 18/60\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8083 - loss: 0.4122 - mse: 0.1334\n",
            "Epoch 18: val_accuracy did not improve from 0.79831\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.8082 - loss: 0.4126 - mse: 0.1336 - val_accuracy: 0.7876 - val_loss: 0.4507 - val_mse: 0.1480\n",
            "Epoch 19/60\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7993 - loss: 0.4159 - mse: 0.1366\n",
            "Epoch 19: val_accuracy did not improve from 0.79831\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.7992 - loss: 0.4162 - mse: 0.1367 - val_accuracy: 0.7952 - val_loss: 0.4369 - val_mse: 0.1424\n",
            "Epoch 20/60\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7974 - loss: 0.4383 - mse: 0.1428\n",
            "Epoch 20: val_accuracy improved from 0.79831 to 0.81967, saving model to unet-attention-4band-lakes.keras\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.7973 - loss: 0.4382 - mse: 0.1428 - val_accuracy: 0.8197 - val_loss: 0.4300 - val_mse: 0.1378\n",
            "Epoch 21/60\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8160 - loss: 0.4164 - mse: 0.1342\n",
            "Epoch 21: val_accuracy did not improve from 0.81967\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.8159 - loss: 0.4164 - mse: 0.1342 - val_accuracy: 0.8062 - val_loss: 0.4300 - val_mse: 0.1382\n",
            "Epoch 22/60\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8116 - loss: 0.4114 - mse: 0.1338\n",
            "Epoch 22: val_accuracy did not improve from 0.81967\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.8113 - loss: 0.4117 - mse: 0.1340 - val_accuracy: 0.8085 - val_loss: 0.4285 - val_mse: 0.1385\n",
            "Epoch 23/60\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7950 - loss: 0.4572 - mse: 0.1490\n",
            "Epoch 23: val_accuracy did not improve from 0.81967\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.7951 - loss: 0.4568 - mse: 0.1489 - val_accuracy: 0.7962 - val_loss: 0.4364 - val_mse: 0.1429\n",
            "Epoch 24/60\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7843 - loss: 0.4612 - mse: 0.1531\n",
            "Epoch 24: val_accuracy did not improve from 0.81967\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.7845 - loss: 0.4607 - mse: 0.1529 - val_accuracy: 0.8087 - val_loss: 0.4372 - val_mse: 0.1376\n",
            "Epoch 25/60\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8146 - loss: 0.3940 - mse: 0.1257\n",
            "Epoch 25: val_accuracy did not improve from 0.81967\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.8145 - loss: 0.3942 - mse: 0.1258 - val_accuracy: 0.7862 - val_loss: 0.4485 - val_mse: 0.1483\n",
            "Epoch 26/60\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8255 - loss: 0.3950 - mse: 0.1267\n",
            "Epoch 26: val_accuracy did not improve from 0.81967\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.8252 - loss: 0.3954 - mse: 0.1269 - val_accuracy: 0.7714 - val_loss: 0.4642 - val_mse: 0.1542\n",
            "Epoch 27/60\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7989 - loss: 0.4285 - mse: 0.1407\n",
            "Epoch 27: val_accuracy did not improve from 0.81967\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.7987 - loss: 0.4286 - mse: 0.1408 - val_accuracy: 0.7661 - val_loss: 0.4627 - val_mse: 0.1539\n",
            "Epoch 28/60\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8100 - loss: 0.4108 - mse: 0.1337\n",
            "Epoch 28: val_accuracy did not improve from 0.81967\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.8100 - loss: 0.4109 - mse: 0.1337 - val_accuracy: 0.7885 - val_loss: 0.4488 - val_mse: 0.1474\n",
            "Epoch 29/60\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7595 - loss: 0.5179 - mse: 0.1633\n",
            "Epoch 29: val_accuracy did not improve from 0.81967\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.7596 - loss: 0.5179 - mse: 0.1634 - val_accuracy: 0.8052 - val_loss: 0.4580 - val_mse: 0.1468\n",
            "Epoch 30/60\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8067 - loss: 0.4514 - mse: 0.1460\n",
            "Epoch 30: val_accuracy did not improve from 0.81967\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.8066 - loss: 0.4512 - mse: 0.1460 - val_accuracy: 0.8185 - val_loss: 0.4254 - val_mse: 0.1362\n",
            "Epoch 31/60\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8040 - loss: 0.4566 - mse: 0.1475\n",
            "Epoch 31: val_accuracy did not improve from 0.81967\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.8042 - loss: 0.4561 - mse: 0.1474 - val_accuracy: 0.7657 - val_loss: 0.4736 - val_mse: 0.1588\n",
            "Epoch 32/60\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7888 - loss: 0.4515 - mse: 0.1492\n",
            "Epoch 32: val_accuracy did not improve from 0.81967\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.7887 - loss: 0.4514 - mse: 0.1491 - val_accuracy: 0.7912 - val_loss: 0.4379 - val_mse: 0.1435\n",
            "Epoch 33/60\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8274 - loss: 0.3855 - mse: 0.1232\n",
            "Epoch 33: val_accuracy did not improve from 0.81967\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.8271 - loss: 0.3859 - mse: 0.1234 - val_accuracy: 0.8115 - val_loss: 0.4271 - val_mse: 0.1373\n",
            "Epoch 34/60\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8146 - loss: 0.4035 - mse: 0.1311\n",
            "Epoch 34: val_accuracy did not improve from 0.81967\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.8143 - loss: 0.4039 - mse: 0.1312 - val_accuracy: 0.8191 - val_loss: 0.4239 - val_mse: 0.1370\n",
            "Epoch 35/60\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7981 - loss: 0.4382 - mse: 0.1438\n",
            "Epoch 35: val_accuracy did not improve from 0.81967\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.7981 - loss: 0.4381 - mse: 0.1437 - val_accuracy: 0.8072 - val_loss: 0.4345 - val_mse: 0.1407\n",
            "Epoch 36/60\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8075 - loss: 0.4135 - mse: 0.1347\n",
            "Epoch 36: val_accuracy did not improve from 0.81967\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.8074 - loss: 0.4137 - mse: 0.1348 - val_accuracy: 0.8012 - val_loss: 0.4327 - val_mse: 0.1405\n",
            "Epoch 37/60\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8066 - loss: 0.4177 - mse: 0.1360\n",
            "Epoch 37: val_accuracy did not improve from 0.81967\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.8067 - loss: 0.4176 - mse: 0.1360 - val_accuracy: 0.8071 - val_loss: 0.4231 - val_mse: 0.1369\n",
            "Epoch 38/60\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8031 - loss: 0.4167 - mse: 0.1370\n",
            "Epoch 38: val_accuracy did not improve from 0.81967\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.8028 - loss: 0.4171 - mse: 0.1371 - val_accuracy: 0.8006 - val_loss: 0.4372 - val_mse: 0.1432\n",
            "Epoch 39/60\n",
            "\u001b[1m 91/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8186 - loss: 0.4062 - mse: 0.1311"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU7s-wvYQgy7"
      },
      "source": [
        "# Save model history\n",
        "np.save('unet-attention-4d-history.npy',model_attention_unet_4band.history.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8jeWN9XfEO-"
      },
      "source": [
        "# Copy models to drive\n",
        "!cp unet-attention-4band-lakes.keras /content/drive/MyDrive/Diss/\n",
        "!cp unet-attention-4d-history.npy drive/MyDrive/Diss/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wb_jgNjdurVl"
      },
      "source": [
        "## 4-band Atlantic Forest dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "azfzKAKfEvNC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANUhzzdQvOaL"
      },
      "source": [
        "# Ingest images and normalise\n",
        "\n",
        "## Training images\n",
        "training_images_list3 = os.listdir(r\"{}Training/image/\".format(base_dir3))[0:250]\n",
        "training_masks_list3 = []\n",
        "training_images3 = []\n",
        "for n in training_images_list3:\n",
        "  training_masks_list3.append(n)\n",
        "  a = (np.array(rxr.open_rasterio(r\"{}Training/image/{}\".format(base_dir3,n))))\n",
        "  a = (a-np.min(a)) / (np.max(a)-np.min(a))\n",
        "  training_images3.append(a)\n",
        "\n",
        "## Training masks\n",
        "training_masks3 = []\n",
        "for n in training_masks_list3:\n",
        "  a = (np.array(rxr.open_rasterio(r\"{}Training/label/{}\".format(base_dir3,n))))\n",
        "  training_masks3.append(a)\n",
        "\n",
        "## Test images\n",
        "test_images_list3 = os.listdir(r\"{}Test/image/\".format(base_dir3))\n",
        "test_masks_list3 = []\n",
        "test_images3 = []\n",
        "for n in test_images_list3:\n",
        "  test_masks_list3.append(n)\n",
        "  a = (np.array(rxr.open_rasterio(r\"{}Test/image/{}\".format(base_dir3,n))))\n",
        "  a = (a-np.min(a)) / (np.max(a)-np.min(a))\n",
        "  test_images3.append(a)\n",
        "\n",
        "## Test masks\n",
        "test_masks3 = []\n",
        "for n in test_masks_list3:\n",
        "  a = (np.array(rxr.open_rasterio(r\"{}Test/mask/{}\".format(base_dir3,n))))\n",
        "  test_masks3.append(a)\n",
        "\n",
        "## Validation images\n",
        "validation_images_list3 = os.listdir(r\"{}Validation/images/\".format(base_dir3))\n",
        "validation_masks_list3 = []\n",
        "validation_images3 = []\n",
        "for n in validation_images_list3:\n",
        "  validation_masks_list3.append(n)\n",
        "  a = (np.array(rxr.open_rasterio(r\"{}Validation/images/{}\".format(base_dir3,n))))\n",
        "  a = (a-np.min(a)) / (np.max(a)-np.min(a))\n",
        "  validation_images3.append(a)\n",
        "\n",
        "## Validation masks\n",
        "validation_masks3 = []\n",
        "for n in validation_masks_list3:\n",
        "  a = (np.array(rxr.open_rasterio(r\"{}Validation/masks/{}\".format(base_dir3,n))))\n",
        "  validation_masks3.append(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L31I43OJxMVU"
      },
      "source": [
        "# Pre-process data, reshaping and transposing\n",
        "for i in range(len(training_images3)):\n",
        "  training_images3[i] = training_images3[i].astype('float32')\n",
        "  training_images3[i] = training_images3[i].T\n",
        "\n",
        "for i in range(len(training_masks3)):\n",
        "  training_masks3[i] = training_masks3[i].reshape(1,512,512,1)\n",
        "  training_masks3[i] = training_masks3[i].T\n",
        "\n",
        "for i in range(len(validation_images3)):\n",
        "  validation_images3[i] = validation_images3[i].astype('float32')\n",
        "  validation_images3[i] = validation_images3[i].T\n",
        "\n",
        "for i in range(len(validation_masks3)):\n",
        "  validation_masks3[i] = validation_masks3[i].reshape(1,512,512,1)\n",
        "  validation_masks3[i] = validation_masks3[i].T\n",
        "\n",
        "for i in range(len(test_images3)):\n",
        "  test_images3[i] = test_images3[i].astype('float32')\n",
        "  test_images3[i] = test_images3[i].T\n",
        "\n",
        "for i in range(len(test_masks3)):\n",
        "  test_masks3[i] = test_masks3[i].reshape(1,512,512,1)\n",
        "  test_masks3[i] = test_masks3[i].T\n",
        "\n",
        "\n",
        "for i in range(len(training_images3)):\n",
        "  training_images3[i] = training_images3[i].reshape(-1,512,512,4)\n",
        "\n",
        "for i in range(len(validation_images3)):\n",
        "  validation_images3[i] = validation_images3[i].reshape(-1,512,512,4)\n",
        "\n",
        "for i in range(len(test_images3)):\n",
        "  test_images3[i] = test_images3[i].reshape(-1,512,512,4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn73cK3QE9fN"
      },
      "source": [
        "# Plot example training image first band\n",
        "plt.imshow(training_images3[0].reshape(512,512,4)[:,:,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n-tBTsjyL5q"
      },
      "source": [
        "# Create TensorFlow datasets for training and validation sets\n",
        "train_df_4band_atlantic = tf.data.Dataset.from_tensor_slices((training_images3[0:250], training_masks3[0:250]))\n",
        "validation_df_4band_atlantic = tf.data.Dataset.from_tensor_slices((validation_images3, validation_masks3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-JXc78rQLaL"
      },
      "source": [
        "# Import Models and Compute Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI5RTFiZD9W-"
      },
      "source": [
        "# **Iraq Lake Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuWpNDuLZS2W"
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "# Load 4-band UNet models\n",
        "unet_4d = load_model('unet-4band-lakes.keras')\n",
        "attention_unet_4d = load_model('unet-attention-4band-lakes.keras')\n",
        "\n",
        "# Load histories\n",
        "unet_4d_history = np.load('unet-4d-history.npy', allow_pickle=True).item()\n",
        "attention_unet_4d_history = np.load('unet-attention-4d-history.npy', allow_pickle=True).item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwDcH3DJQLaO"
      },
      "source": [
        "# Plot accuracy and loss for U-Net\n",
        "\n",
        "## Accuracy\n",
        "plt.plot(unet_4d_history['accuracy'])\n",
        "plt.plot(unet_4d_history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "## Loss\n",
        "plt.plot(unet_4d_history['loss'])\n",
        "plt.plot(unet_4d_history['val_loss'])\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mcn0YjbZpPL"
      },
      "source": [
        "# Plot accuracy and loss for Attention U-Net\n",
        "\n",
        "## Accuracy\n",
        "plt.plot(attention_unet_4d_history['accuracy'])\n",
        "plt.plot(attention_unet_4d_history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "## Loss\n",
        "plt.plot(attention_unet_4d_history['loss'])\n",
        "plt.plot(attention_unet_4d_history['val_loss'])\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import numpy as np\n",
        "\n",
        "def score_eval2(model, images, masks):\n",
        "    \"\"\"\n",
        "    Evaluate model on a whole batch of images + masks.\n",
        "    images: (N, H, W, C)\n",
        "    masks:  (N, H, W) or (N, H, W, 1)\n",
        "    \"\"\"\n",
        "    # Predict on all images\n",
        "    preds = model.predict(images)\n",
        "\n",
        "    # Drop channel dimension if it's (N, H, W, 1)\n",
        "    if preds.ndim == 4 and preds.shape[-1] == 1:\n",
        "        preds = preds[..., 0]\n",
        "    if masks.ndim == 4 and masks.shape[-1] == 1:\n",
        "        masks = masks[..., 0]\n",
        "\n",
        "    # Binarise predictions (0/1)\n",
        "    preds_bin = np.round(preds).astype(np.uint8)\n",
        "\n",
        "    # Flatten everything to 1D for global scores\n",
        "    y_pred = preds_bin.flatten()\n",
        "    y_true = masks.astype(np.uint8).flatten()\n",
        "\n",
        "    # Sanity check\n",
        "    assert y_pred.shape == y_true.shape, f\"Shape mismatch: {y_pred.shape} vs {y_true.shape}\"\n",
        "\n",
        "    scores = {\n",
        "        \"accuracy\":  accuracy_score(y_true, y_pred),\n",
        "        \"f1\":        f1_score(y_true, y_pred),\n",
        "        \"precision\": precision_score(y_true, y_pred),\n",
        "        \"recall\":    recall_score(y_true, y_pred),\n",
        "    }\n",
        "    return scores\n"
      ],
      "metadata": {
        "id": "vrRfgpa9Vgrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1V4qoM4ZtjE"
      },
      "source": [
        "# Scores of each model\n",
        "unet_4d_score = (score_eval2(unet_4d, validation_images2, validation_masks2))\n",
        "am_unet_4d_score = (score_eval2(attention_unet_4d, validation_images2, validation_masks2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "import numpy as np\n",
        "\n",
        "def precision_eval(model, images, masks):\n",
        "    \"\"\"\n",
        "    Global precision over a batch of images.\n",
        "    images: (N, H, W, C)\n",
        "    masks:  (N, H, W) or (N, H, W, 1)\n",
        "    \"\"\"\n",
        "    preds = model.predict(images)\n",
        "\n",
        "    # Drop channel dim if present\n",
        "    if preds.ndim == 4 and preds.shape[-1] == 1:\n",
        "        preds = preds[..., 0]\n",
        "    if masks.ndim == 4 and masks.shape[-1] == 1:\n",
        "        masks = masks[..., 0]\n",
        "\n",
        "    # Binarise predictions\n",
        "    preds_bin = np.round(preds).astype(np.uint8)\n",
        "\n",
        "    # Flatten\n",
        "    y_pred = preds_bin.flatten()\n",
        "    y_true = masks.astype(np.uint8).flatten()\n",
        "\n",
        "    assert y_pred.shape == y_true.shape, f\"Shape mismatch: {y_pred.shape} vs {y_true.shape}\"\n",
        "\n",
        "    return precision_score(y_true, y_pred)\n",
        "\n",
        "\n",
        "def recall_eval(model, images, masks):\n",
        "    \"\"\"\n",
        "    Global recall over a batch of images.\n",
        "    images: (N, H, W, C)\n",
        "    masks:  (N, H, W) or (N, H, W, 1)\n",
        "    \"\"\"\n",
        "    preds = model.predict(images)\n",
        "\n",
        "    # Drop channel dim if present\n",
        "    if preds.ndim == 4 and preds.shape[-1] == 1:\n",
        "        preds = preds[..., 0]\n",
        "    if masks.ndim == 4 and masks.shape[-1] == 1:\n",
        "        masks = masks[..., 0]\n",
        "\n",
        "    # Binarise predictions\n",
        "    preds_bin = np.round(preds).astype(np.uint8)\n",
        "\n",
        "    # Flatten\n",
        "    y_pred = preds_bin.flatten()\n",
        "    y_true = masks.astype(np.uint8).flatten()\n",
        "\n",
        "    assert y_pred.shape == y_true.shape, f\"Shape mismatch: {y_pred.shape} vs {y_true.shape}\"\n",
        "\n",
        "    return recall_score(y_true, y_pred)\n"
      ],
      "metadata": {
        "id": "ZQbq9sw0V0Sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVcI_yk4Z80x"
      },
      "source": [
        "# Precision and recall of each model\n",
        "unet_4d_precision = (precision_eval(unet_4d, validation_images2, validation_masks2))\n",
        "am_unet_4d_precision = (precision_eval(attention_unet_4d, validation_images2, validation_masks2))\n",
        "\n",
        "unet_4d_recall = (recall_eval(unet_4d, validation_images2, validation_masks2))\n",
        "am_unet_4d_recall = (recall_eval(attention_unet_4d, validation_images2, validation_masks2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvarhjO1aTX_"
      },
      "source": [
        "# F1-scores of each model\n",
        "unet_4d_f1_score = (f1_score_eval_basic(unet_4d_precision, unet_4d_recall))\n",
        "am_unet_4d_f1_score = (f1_score_eval_basic(am_unet_4d_precision, am_unet_4d_recall))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Htyl7Pi-aU88"
      },
      "source": [
        "print(\"UNet accuracy:\", unet_4d_score[\"accuracy\"])\n",
        "print(\"UNet f1:\", unet_4d_score[\"f1\"])\n",
        "print(\"UNet precision:\", unet_4d_score[\"precision\"])\n",
        "print(\"UNet recall:\", unet_4d_score[\"recall\"])\n",
        "\n",
        "print(\"\\nAttention UNet accuracy:\", am_unet_4d_score[\"accuracy\"])\n",
        "print(\"Attention UNet f1:\", am_unet_4d_score[\"f1\"])\n",
        "print(\"Attention UNet precision:\", am_unet_4d_score[\"precision\"])\n",
        "print(\"Attention UNet recall:\", am_unet_4d_score[\"recall\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geaOxWrpaWi9"
      },
      "source": [
        "# Print precision eval results for each model\n",
        "print('U-Net precision: ', np.mean(unet_4d_precision), np.std(unet_4d_precision))\n",
        "print('Attention U-Net precision: ', np.mean(am_unet_4d_precision), np.std(am_unet_4d_precision))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mpmu_7EaYEH"
      },
      "source": [
        "# Print recall eval results for each model\n",
        "print('U-Net recall: ', np.mean(unet_4d_recall), np.std(unet_4d_recall))\n",
        "print('Attention U-Net recall: ', np.mean(am_unet_4d_recall), np.std(am_unet_4d_recall))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0NXQpR2aYZG"
      },
      "source": [
        "# Print f1-score eval results for each model\n",
        "print('U-Net F1-score: ', np.mean(unet_4d_f1_score))\n",
        "print('Attention U-Net F1-score: ', np.mean(am_unet_4d_f1_score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beZYrcZdzE0c"
      },
      "source": [
        "### Amazon on unseen Atlantic data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7LTlQvAzH8K"
      },
      "source": [
        "# Score\n",
        "unet_amazon_on_atlantic_score = score_eval2(unet_4d, validation_images3+test_images3, validation_masks3+test_masks3)\n",
        "am_unet_amazon_on_atlantic_score = score_eval2(attention_unet_4d, validation_images3+test_images3, validation_masks3+test_masks3)\n",
        "\n",
        "# Precision\n",
        "unet_amazon_on_atlantic_precision = (precision_eval(unet_4d, validation_images3+test_images3, validation_masks3+test_masks3))\n",
        "am_unet_amazon_on_atlantic_precision = (precision_eval(attention_unet_4d, validation_images3+test_images3, validation_masks3+test_masks3))\n",
        "\n",
        "# Recall\n",
        "unet_amazon_on_atlantic_recall = (recall_eval(unet_4d, validation_images3+test_images3, validation_masks3+test_masks3))\n",
        "am_unet_amazon_on_atlantic_recall = (recall_eval(attention_unet_4d, validation_images3+test_images3, validation_masks3+test_masks3))\n",
        "\n",
        "# F1-scores of each model\n",
        "unet_amazon_on_atlantic_f1_score = (f1_score_eval_basic(unet_amazon_on_atlantic_precision, unet_amazon_on_atlantic_recall))\n",
        "am_unet_amazon_on_atlantic_f1_score = (f1_score_eval_basic(am_unet_amazon_on_atlantic_precision, am_unet_amazon_on_atlantic_recall))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtAbkWSY0Xnn"
      },
      "source": [
        "# Print metrics\n",
        "print('U-Net score: ', np.mean(unet_amazon_on_atlantic_score), np.std(unet_amazon_on_atlantic_score))\n",
        "print('Attention U-Net score: ', np.mean(am_unet_amazon_on_atlantic_score), np.std(am_unet_amazon_on_atlantic_score))\n",
        "\n",
        "print('U-Net precision: ', np.mean(unet_amazon_on_atlantic_precision), np.std(unet_amazon_on_atlantic_precision))\n",
        "print('Attention U-Net precision: ', np.mean(am_unet_amazon_on_atlantic_precision), np.std(am_unet_amazon_on_atlantic_precision))\n",
        "\n",
        "print('U-Net recall: ', np.mean(unet_amazon_on_atlantic_recall), np.std(unet_amazon_on_atlantic_recall))\n",
        "print('Attention U-Net recall: ', np.mean(am_unet_amazon_on_atlantic_recall), np.std(am_unet_amazon_on_atlantic_recall))\n",
        "\n",
        "print('U-Net F1-score: ', unet_amazon_on_atlantic_f1_score)\n",
        "print('Attention U-Net F1-score: ', am_unet_amazon_on_atlantic_f1_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRQOG9uUECcu"
      },
      "source": [
        "### Atlantic Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQIc0XDmEFND"
      },
      "source": [
        "# Load 4-dim models and history stats\n",
        "attention_unet_4d_atlantic = load_model('unet-attention-4d-atlantic.keras')\n",
        "unet_4d_atlantic = load_model('unet-4d-atlantic.keras')\n",
        "\n",
        "unet_4d_atlantic_history = np.load('unet-4d-atlantic-history.npy', allow_pickle='TRUE').item()\n",
        "attention_unet_4d_atlantic_history = np.load('unet-attention-4d-atlantic-history.npy', allow_pickle='TRUE').item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4YBSQxjEWLc"
      },
      "source": [
        "# Plot accuracy and loss for U-Net\n",
        "\n",
        "## Accuracy\n",
        "plt.plot(unet_4d_atlantic_history['accuracy'])\n",
        "plt.plot(unet_4d_atlantic_history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "## Loss\n",
        "plt.plot(unet_4d_atlantic_history['loss'])\n",
        "plt.plot(unet_4d_atlantic_history['val_loss'])\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCFSJmMzEaR5"
      },
      "source": [
        "# Plot accuracy and loss for Attention U-Net\n",
        "\n",
        "## Accuracy\n",
        "plt.plot(attention_unet_4d_atlantic_history['accuracy'])\n",
        "plt.plot(attention_unet_4d_atlantic_history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "## Loss\n",
        "plt.plot(attention_unet_4d_atlantic_history['loss'])\n",
        "plt.plot(attention_unet_4d_atlantic_history['val_loss'])\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNxllVnXEhvb"
      },
      "source": [
        "# Scores of each model\n",
        "unet_4d_atlantic_score = (score_eval2(unet_4d_atlantic, validation_images3, validation_masks3))\n",
        "am_unet_4d_atlantic_score = (score_eval2(attention_unet_4d_atlantic, validation_images3, validation_masks3))\n",
        "\n",
        "# Precision and recall of each model\n",
        "unet_4d_atlantic_precision = (precision_eval(unet_4d_atlantic, validation_images3, validation_masks3))\n",
        "am_unet_4d_atlantic_precision = (precision_eval(attention_unet_4d_atlantic, validation_images3, validation_masks3))\n",
        "\n",
        "unet_4d_atlantic_recall = (recall_eval(unet_4d_atlantic, validation_images3, validation_masks3))\n",
        "am_unet_4d_atlantic_recall = (recall_eval(attention_unet_4d_atlantic, validation_images3, validation_masks3))\n",
        "\n",
        "# F1-scores of each model\n",
        "unet_4d_atlantic_f1_score = (f1_score_eval_basic(unet_4d_atlantic_precision, unet_4d_atlantic_recall))\n",
        "am_unet_4d_atlantic_f1_score = (f1_score_eval_basic(am_unet_4d_atlantic_precision, am_unet_4d_atlantic_recall))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuoQ8__mE9Ut"
      },
      "source": [
        "# Print metrics\n",
        "print('U-Net score: ', np.mean(unet_4d_atlantic_score), np.std(unet_4d_atlantic_score))\n",
        "print('Attention U-Net score: ', np.mean(am_unet_4d_atlantic_score), np.std(am_unet_4d_atlantic_score))\n",
        "\n",
        "print('U-Net precision: ', np.mean(unet_4d_atlantic_precision), np.std(unet_4d_atlantic_precision))\n",
        "print('Attention U-Net precision: ', np.mean(am_unet_4d_atlantic_precision), np.std(am_unet_4d_atlantic_precision))\n",
        "\n",
        "print('U-Net recall: ', np.mean(unet_4d_atlantic_recall), np.std(unet_4d_atlantic_recall))\n",
        "print('Attention U-Net recall: ', np.mean(am_unet_4d_atlantic_recall), np.std(am_unet_4d_atlantic_recall))\n",
        "\n",
        "print('U-Net F1-score: ', unet_4d_atlantic_f1_score)\n",
        "print('Attention U-Net F1-score: ', am_unet_4d_atlantic_f1_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LttV4BY-FO9Y"
      },
      "source": [
        "### Atlantic on unseen Amazon data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0mUcfs7FWCn"
      },
      "source": [
        "# Score\n",
        "unet_atlantic_on_amazon_score = score_eval2(unet_4d_atlantic, validation_images2+test_images2, validation_masks2+test_masks2)\n",
        "am_unet_atlantic_on_amazon_score = score_eval2(attention_unet_4d_atlantic, validation_images2+test_images2, validation_masks2+test_masks2)\n",
        "\n",
        "# Precision\n",
        "unet_atlantic_on_amazon_precision = (precision_eval(unet_4d_atlantic, validation_images2+test_images2, validation_masks2+test_masks2))\n",
        "am_unet_atlantic_on_amazon_precision = (precision_eval(attention_unet_4d_atlantic, validation_images2+test_images2, validation_masks2+test_masks2))\n",
        "\n",
        "# Recall\n",
        "unet_atlantic_on_amazon_recall = (recall_eval(unet_4d_atlantic, validation_images2+test_images2, validation_masks2+test_masks2))\n",
        "am_unet_atlantic_on_amazon_recall = (recall_eval(attention_unet_4d_atlantic, validation_images2+test_images2, validation_masks2+test_masks2))\n",
        "\n",
        "# F1-scores of each model\n",
        "unet_atlantic_on_amazon_f1_score = (f1_score_eval_basic(unet_atlantic_on_amazon_precision, unet_atlantic_on_amazon_recall))\n",
        "am_unet_atlantic_on_amazon_f1_score = (f1_score_eval_basic(am_unet_atlantic_on_amazon_precision, am_unet_atlantic_on_amazon_recall))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emaoDHfdFWfL"
      },
      "source": [
        "# Print metrics\n",
        "print('U-Net score: ', np.mean(unet_atlantic_on_amazon_score), np.std(unet_atlantic_on_amazon_score))\n",
        "print('Attention U-Net score: ', np.mean(am_unet_atlantic_on_amazon_score), np.std(am_unet_atlantic_on_amazon_score))\n",
        "\n",
        "print('U-Net precision: ', np.mean(unet_atlantic_on_amazon_precision), np.std(unet_atlantic_on_amazon_precision))\n",
        "print('Attention U-Net precision: ', np.mean(am_unet_atlantic_on_amazon_precision), np.std(am_unet_atlantic_on_amazon_precision))\n",
        "\n",
        "print('U-Net recall: ', np.mean(unet_atlantic_on_amazon_recall), np.std(unet_atlantic_on_amazon_recall))\n",
        "print('Attention U-Net recall: ', np.mean(am_unet_atlantic_on_amazon_recall), np.std(am_unet_atlantic_on_amazon_recall))\n",
        "\n",
        "print('U-Net F1-score: ', unet_atlantic_on_amazon_f1_score)\n",
        "print('Attention U-Net F1-score: ', am_unet_atlantic_on_amazon_f1_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EWGiqFzlsrv"
      },
      "source": [
        "### Amazon and Atlantic unseen test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Txs1Av6GpAlu"
      },
      "source": [
        "# Amazon trained model on Amazon test data\n",
        "# Scores of each model\n",
        "unet_4d_score_test = (score_eval2(unet_4d, test_images2, test_masks2))\n",
        "am_unet_4d_score_test = (score_eval2(attention_unet_4d, test_images2, test_masks2))\n",
        "\n",
        "# Precision and recall of each model\n",
        "unet_4d_precision_test = (precision_eval(unet_4d, test_images2, test_masks2))\n",
        "am_unet_4d_precision_test = (precision_eval(attention_unet_4d, test_images2, test_masks2))\n",
        "\n",
        "unet_4d_recall_test = (recall_eval(unet_4d, test_images2, test_masks2))\n",
        "am_unet_4d_recall_test = (recall_eval(attention_unet_4d, test_images2, test_masks2))\n",
        "\n",
        "# F1-scores of each model\n",
        "unet_4d_f1_score_test = (f1_score_eval_basic(unet_4d_precision_test, unet_4d_recall_test))\n",
        "am_unet_4d_f1_score_test = (f1_score_eval_basic(am_unet_4d_precision_test, am_unet_4d_recall_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Njk-zCC7lw8_"
      },
      "source": [
        "# Atlantic trained model on Atlantic test data\n",
        "# Scores of each model\n",
        "unet_4d_atlantic_score_test = (score_eval2(unet_4d_atlantic, test_images3, test_masks3))\n",
        "am_unet_4d_atlantic_score_test = (score_eval2(attention_unet_4d_atlantic, test_images3, test_masks3))\n",
        "\n",
        "# Precision and recall of each model\n",
        "unet_4d_atlantic_precision_test = (precision_eval(unet_4d_atlantic, test_images3, test_masks3))\n",
        "am_unet_4d_atlantic_precision_test = (precision_eval(attention_unet_4d_atlantic, test_images3, test_masks3))\n",
        "\n",
        "unet_4d_atlantic_recall_test = (recall_eval(unet_4d_atlantic, test_images3, test_masks3))\n",
        "am_unet_4d_atlantic_recall_test = (recall_eval(attention_unet_4d_atlantic, test_images3, test_masks3))\n",
        "\n",
        "# F1-scores of each model\n",
        "unet_4d_atlantic_f1_score_test = (f1_score_eval_basic(unet_4d_atlantic_precision_test, unet_4d_atlantic_recall_test))\n",
        "am_unet_4d_atlantic_f1_score_test = (f1_score_eval_basic(am_unet_4d_atlantic_precision_test, am_unet_4d_atlantic_recall_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSCJGrpkppSA"
      },
      "source": [
        "# Print metrics for Amazon on Amazon Test set\n",
        "print('U-Net score: ', np.mean(unet_4d_score_test), np.std(unet_4d_score_test))\n",
        "print('Attention U-Net score: ', np.mean(am_unet_4d_score_test), np.std(am_unet_4d_score_test))\n",
        "\n",
        "print('U-Net precision: ', np.mean(unet_4d_precision_test), np.std(unet_4d_precision_test))\n",
        "print('Attention U-Net precision: ', np.mean(am_unet_4d_precision_test), np.std(am_unet_4d_precision_test))\n",
        "\n",
        "print('U-Net recall: ', np.mean(unet_4d_recall_test), np.std(unet_4d_recall_test))\n",
        "print('Attention U-Net recall: ', np.mean(am_unet_4d_recall_test), np.std(am_unet_4d_recall_test))\n",
        "\n",
        "print('U-Net F1-score: ', unet_4d_f1_score_test)\n",
        "print('Attention U-Net F1-score: ', am_unet_4d_f1_score_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnXkLvzuqpYv"
      },
      "source": [
        "# Print metrics for Atlantic on Atlantic Test set\n",
        "print('U-Net score: ', np.mean(unet_4d_atlantic_score_test), np.std(unet_4d_atlantic_score_test))\n",
        "print('Attention U-Net score: ', np.mean(am_unet_4d_atlantic_score_test), np.std(am_unet_4d_atlantic_score_test))\n",
        "\n",
        "print('U-Net precision: ', np.mean(unet_4d_atlantic_precision_test), np.std(unet_4d_atlantic_precision_test))\n",
        "print('Attention U-Net precision: ', np.mean(am_unet_4d_atlantic_precision_test), np.std(am_unet_4d_atlantic_precision_test))\n",
        "\n",
        "print('U-Net recall: ', np.mean(unet_4d_atlantic_recall_test), np.std(unet_4d_atlantic_recall_test))\n",
        "print('Attention U-Net recall: ', np.mean(am_unet_4d_atlantic_recall_test), np.std(am_unet_4d_atlantic_recall_test))\n",
        "\n",
        "print('U-Net F1-score: ', unet_4d_atlantic_f1_score_test)\n",
        "print('Attention U-Net F1-score: ', am_unet_4d_atlantic_f1_score_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PIo2l0n1XOt"
      },
      "source": [
        "# Produce metric datasets for export"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hj2L_RE7149Q"
      },
      "source": [
        "## 4-band Amazon data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMbXcGo31682"
      },
      "source": [
        "scores_4d = [unet_4d_score, am_unet_4d_score]\n",
        "precision_4d = [unet_4d_precision, am_unet_4d_precision]\n",
        "recall_4d = [unet_4d_recall, am_unet_4d_recall]\n",
        "f1_scores_4d = [unet_4d_f1_score, am_unet_4d_f1_score]\n",
        "\n",
        "metrics_4d = {'classifier': ['U-Net', 'Attention U-Net'],\n",
        "              'accuracy': [np.mean(n) for n in scores_4d],\n",
        "              'precision': [np.mean(n) for n in precision_4d],\n",
        "              'recall': [np.mean(n) for n in recall_4d],\n",
        "              'f1_score': [np.mean(n) for n in f1_scores_4d],\n",
        "              'accuracy_std': [np.std(n) for n in scores_4d],\n",
        "              'precision_std': [np.std(n) for n in precision_4d],\n",
        "              'recall_std': [np.std(n) for n in recall_4d]\n",
        "              }\n",
        "metrics_4d = pd.DataFrame(metrics_4d)\n",
        "metrics_4d.to_csv('metrics_4d_amazon.csv')\n",
        "metrics_4d.to_csv('/content/drive/MyDrive/Diss/metrics_4d_amazon.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eYZ0RkN17XE"
      },
      "source": [
        "## 4-band Atlantic Forest data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edIeklLV19Zi"
      },
      "source": [
        "scores_4d_atl = [unet_4d_atlantic_score, am_unet_4d_atlantic_score]\n",
        "precision_4d_atl = [unet_4d_atlantic_precision, am_unet_4d_atlantic_precision]\n",
        "recall_4d_atl = [unet_4d_atlantic_recall, am_unet_4d_atlantic_recall]\n",
        "f1_scores_4d_atl = [unet_4d_atlantic_f1_score, am_unet_4d_atlantic_f1_score]\n",
        "\n",
        "metrics_4d_atl = {'classifier': ['U-Net', 'Attention U-Net'],\n",
        "              'accuracy': [np.mean(n) for n in scores_4d_atl],\n",
        "              'precision': [np.mean(n) for n in precision_4d_atl],\n",
        "              'recall': [np.mean(n) for n in recall_4d_atl],\n",
        "              'f1_score': [np.mean(n) for n in f1_scores_4d_atl],\n",
        "              'accuracy_std': [np.std(n) for n in scores_4d_atl],\n",
        "              'precision_std': [np.std(n) for n in precision_4d_atl],\n",
        "              'recall_std': [np.std(n) for n in recall_4d_atl]\n",
        "              }\n",
        "metrics_4d_atl = pd.DataFrame(metrics_4d_atl)\n",
        "metrics_4d_atl.to_csv('metrics_4d_atlantic_forest.csv')\n",
        "metrics_4d_atl.to_csv('/content/drive/MyDrive/Diss/metrics_4d_atlantic_forest.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF7HQghy5Qxz"
      },
      "source": [
        "## Test set data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCt9VZ3s5SSr"
      },
      "source": [
        "scores_4d_test = [unet_4d_score_test, am_unet_4d_score_test]\n",
        "precision_4d_test = [unet_4d_precision_test, am_unet_4d_precision_test]\n",
        "recall_4d_test = [unet_4d_recall_test, am_unet_4d_recall_test]\n",
        "f1_scores_4d_test = [unet_4d_f1_score_test, am_unet_4d_f1_score_test]\n",
        "\n",
        "metrics_4d_test = {'classifier': ['U-Net', 'Attention U-Net'],\n",
        "              'accuracy': [np.mean(n) for n in scores_4d_test],\n",
        "              'precision': [np.mean(n) for n in precision_4d_test],\n",
        "              'recall': [np.mean(n) for n in recall_4d_test],\n",
        "              'f1_score': [np.mean(n) for n in f1_scores_4d_test],\n",
        "              'accuracy_std': [np.std(n) for n in scores_4d_test],\n",
        "              'precision_std': [np.std(n) for n in precision_4d_test],\n",
        "              'recall_std': [np.std(n) for n in recall_4d_test]\n",
        "              }\n",
        "metrics_4d_test = pd.DataFrame(metrics_4d_test)\n",
        "metrics_4d_test.to_csv('metrics_4d_amazon_test.csv')\n",
        "metrics_4d_test.to_csv('/content/drive/MyDrive/Diss/metrics_4d_amazon_test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M127Qw5A5xUX"
      },
      "source": [
        "scores_4d_atl_test = [unet_4d_atlantic_score_test, am_unet_4d_atlantic_score_test]\n",
        "precision_4d_atl_test = [unet_4d_atlantic_precision_test, am_unet_4d_atlantic_precision_test]\n",
        "recall_4d_atl_test = [unet_4d_atlantic_recall_test, am_unet_4d_atlantic_recall_test]\n",
        "f1_scores_4d_atl_test = [unet_4d_atlantic_f1_score_test, am_unet_4d_atlantic_f1_score_test]\n",
        "\n",
        "metrics_4d_atl_test = {'classifier': ['U-Net', 'Attention U-Net'],\n",
        "              'accuracy': [np.mean(n) for n in scores_4d_atl_test],\n",
        "              'precision': [np.mean(n) for n in precision_4d_atl_test],\n",
        "              'recall': [np.mean(n) for n in recall_4d_atl_test],\n",
        "              'f1_score': [np.mean(n) for n in f1_scores_4d_atl_test],\n",
        "              'accuracy_std': [np.std(n) for n in scores_4d_atl_test],\n",
        "              'precision_std': [np.std(n) for n in precision_4d_atl_test],\n",
        "              'recall_std': [np.std(n) for n in recall_4d_atl_test]\n",
        "              }\n",
        "metrics_4d_atl_test = pd.DataFrame(metrics_4d_atl_test)\n",
        "metrics_4d_atl_test.to_csv('metrics_4d_atlantic_forest_test.csv')\n",
        "metrics_4d_atl_test.to_csv('/content/drive/MyDrive/Diss/metrics_4d_atlantic_forest_test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYkobMR86TR6"
      },
      "source": [
        "## Testing on opposite dataset (e.g. train on Amazon, test on Atlantic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKl_IU1q4yeE"
      },
      "source": [
        "## Train on 4-band Atlantic data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDa_hakb42II"
      },
      "source": [
        "### U-Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULAHseJg403h"
      },
      "source": [
        "# Train U-Net with generator\n",
        "model_unet_4band_atlantic = UNet(input_size=(512, 512, 4), lr=0.0001)\n",
        "save_model_4band_atlantic = ModelCheckpoint('unet-4d-atlantic.keras', monitor='val_accuracy',verbose=1, save_best_only=True)\n",
        "model_unet_4band_atlantic.fit(train_df_4band_atlantic, epochs = 20, validation_data = validation_df_4band_atlantic, callbacks=[save_model_4band_atlantic])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saTWiRbk5NLN"
      },
      "source": [
        "# Save model history\n",
        "np.save('unet-4d-atlantic-history.npy',model_unet_4band_atlantic.history.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNIQWXYMoNi7"
      },
      "source": [
        "# Copy models to drive\n",
        "!cp unet-4d-atlantic.keras drive/MyDrive/Diss/\n",
        "!cp unet-4d-atlantic-history.npy drive/MyDrive/Diss/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieuJPBse44cA"
      },
      "source": [
        "### Attention U-Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEvQIolulSNp"
      },
      "source": [
        "# Train Attention U-Net with generator\n",
        "model_attention_unet_4band_atlantic = UNetAM(input_size=(512,512,4), filter_base=16, lr=0.0005)\n",
        "save_model_4band_attention_atlantic = ModelCheckpoint('unet-attention-4d-atlantic.keras', monitor='val_accuracy',verbose=1, save_best_only=True)\n",
        "model_attention_unet_4band_atlantic.fit(train_df_4band_atlantic, epochs = 60, validation_data = validation_df_4band_atlantic, callbacks=[save_model_4band_attention_atlantic])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vYhP4_25caq"
      },
      "source": [
        "# Save model history\n",
        "np.save('unet-attention-4d-atlantic-history.npy',model_attention_unet_4band_atlantic.history.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMCaWw6On6_D"
      },
      "source": [
        "# Copy models to drive\n",
        "!cp unet-attention-4d-atlantic.keras drive/MyDrive/Diss/\n",
        "!cp unet-attention-4d-atlantic-history.npy drive/MyDrive/Diss/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYz22lx-6X3V"
      },
      "source": [
        "scores_amazon_on_atlantic = [unet_amazon_on_atlantic_score, am_unet_amazon_on_atlantic_score]\n",
        "precision_amazon_on_atlantic = [unet_amazon_on_atlantic_precision, am_unet_amazon_on_atlantic_precision]\n",
        "recall_amazon_on_atlantic = [unet_amazon_on_atlantic_recall, am_unet_amazon_on_atlantic_recall]\n",
        "f1_scores_amazon_on_atlantic = [unet_amazon_on_atlantic_f1_score, am_unet_amazon_on_atlantic_f1_score]\n",
        "\n",
        "metrics_4d_amazon_on_atlantic = {'classifier': ['U-Net', 'Attention U-Net'],\n",
        "              'accuracy': [np.mean(n) for n in scores_amazon_on_atlantic],\n",
        "              'precision': [np.mean(n) for n in precision_amazon_on_atlantic],\n",
        "              'recall': [np.mean(n) for n in recall_amazon_on_atlantic],\n",
        "              'f1_score': [np.mean(n) for n in f1_scores_amazon_on_atlantic],\n",
        "              'accuracy_std': [np.std(n) for n in scores_amazon_on_atlantic],\n",
        "              'precision_std': [np.std(n) for n in precision_amazon_on_atlantic],\n",
        "              'recall_std': [np.std(n) for n in recall_amazon_on_atlantic]\n",
        "              }\n",
        "metrics_4d_amazon_on_atlantic = pd.DataFrame(metrics_4d_amazon_on_atlantic)\n",
        "metrics_4d_amazon_on_atlantic.to_csv('metrics_4d_amazon_on_atlantic.csv')\n",
        "metrics_4d_amazon_on_atlantic.to_csv('/content/drive/MyDrive/Diss/metrics_4d_amazon_on_atlantic.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OunQa23t7mwA"
      },
      "source": [
        "scores_atlantic_on_amazon = [unet_atlantic_on_amazon_score, am_unet_atlantic_on_amazon_score]\n",
        "precision_atlantic_on_amazon = [unet_atlantic_on_amazon_precision, am_unet_atlantic_on_amazon_precision]\n",
        "recall_atlantic_on_amazon = [unet_atlantic_on_amazon_recall, am_unet_atlantic_on_amazon_recall]\n",
        "f1_scores_atlantic_on_amazon = [unet_atlantic_on_amazon_f1_score, am_unet_atlantic_on_amazon_f1_score]\n",
        "\n",
        "metrics_4d_atlantic_on_amazon = {'classifier': ['U-Net', 'Attention U-Net'],\n",
        "              'accuracy': [np.mean(n) for n in scores_atlantic_on_amazon],\n",
        "              'precision': [np.mean(n) for n in precision_atlantic_on_amazon],\n",
        "              'recall': [np.mean(n) for n in recall_atlantic_on_amazon],\n",
        "              'f1_score': [np.mean(n) for n in f1_scores_atlantic_on_amazon],\n",
        "              'accuracy_std': [np.std(n) for n in scores_atlantic_on_amazon],\n",
        "              'precision_std': [np.std(n) for n in precision_atlantic_on_amazon],\n",
        "              'recall_std': [np.std(n) for n in recall_atlantic_on_amazon]\n",
        "              }\n",
        "metrics_4d_atlantic_on_amazon = pd.DataFrame(metrics_4d_atlantic_on_amazon)\n",
        "metrics_4d_atlantic_on_amazon.to_csv('metrics_4d_atlantic_on_amazon.csv')\n",
        "# Save CSV directly to Google Drive\n",
        "metrics_4d_atlantic_on_amazon.to_csv('/content/drive/MyDrive/Diss/metrics_4d_atlantic_on_amazon.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l /content\n"
      ],
      "metadata": {
        "id": "F-OuU--Wucot"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}